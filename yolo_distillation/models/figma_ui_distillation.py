"""
Figma UI ê²€ì¦ì„ ìœ„í•œ YOLOv11 ì¦ë¥˜ ë©”ì¸ í´ë˜ìŠ¤ (ë¦¬íŒ©í† ë§ ë²„ì „)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from ultralytics import YOLO
import yaml
from pathlib import Path
from typing import Dict, Tuple, Optional, List
import numpy as np
import wandb
from datetime import datetime
import cv2
import os
from torch.utils.data import DataLoader
import multiprocessing as mp
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from PIL import Image
import random
import time

from ..losses.single_class_loss import SingleClassDistillationLoss
from ..losses.feature_alignment_loss import FeatureAlignmentLoss


class FigmaUIDistillation:
    """Figma UI ê²€ì¦ì„ ìœ„í•œ YOLOv11 ì¦ë¥˜ ë©”ì¸ í´ë˜ìŠ¤ (ë¦¬íŒ©í† ë§ ë²„ì „)"""
    
    def __init__(self,
                 teacher_model: str = 'yolov11l.pt',
                 student_model: str = 'yolov11s.yaml',
                 data_yaml: str = 'figma_ui.yaml',
                 device: str = 'auto',
                 use_wandb: bool = True):
        """
        Args:
            teacher_model: Teacher ëª¨ë¸ ê²½ë¡œ (YOLOv11-l)
            student_model: Student ëª¨ë¸ ì„¤ì • (YOLOv11-s ë˜ëŠ” YOLOv11-m)
            data_yaml: Figma UI ë°ì´í„°ì…‹ ì„¤ì •
            device: í•™ìŠµ ë””ë°”ì´ìŠ¤ ('auto', 'cuda', 'mps', 'cpu')
            use_wandb: WandB ì‚¬ìš© ì—¬ë¶€
        """
        # ë””ë°”ì´ìŠ¤ ìë™ ì„ íƒ
        self.device = self._select_device(device)
        self.use_wandb = use_wandb
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        print(f"Loading teacher model: {teacher_model}")
        self.teacher = YOLO(teacher_model)
        self.teacher.model = self.teacher.model.to(self.device)
        self.teacher.model.eval()
        for param in self.teacher.model.parameters():
            param.requires_grad = False
        
        print(f"Initializing student model: {student_model}")
        self.student = YOLO(student_model)
        self.student.model = self.student.model.to(self.device)
        self.student.model.train()  # í•™ìŠµ ëª¨ë“œ ëª…ì‹œì  ì„¤ì •
        
        # Student ëª¨ë¸ íŒŒë¼ë¯¸í„° gradient í™œì„±í™” í™•ì¸
        for param in self.student.model.parameters():
            param.requires_grad = True
        
        print(f"âœ… Models loaded on device: {self.device}")
        print(f"âœ… Teacher parameters: {sum(p.numel() for p in self.teacher.model.parameters()):,}")
        print(f"âœ… Student parameters: {sum(p.numel() for p in self.student.model.parameters()):,}")
        
        # ë°ì´í„°ì…‹ ì„¤ì • ë¡œë“œ
        self.load_dataset_config(data_yaml)
        
        # Knowledge Distillation ì†ì‹¤ í•¨ìˆ˜ë“¤
        self.distillation_loss = SingleClassDistillationLoss(
            alpha=0.5, beta=0.5, temperature=4.0, device=self.device
        )
        self.feature_loss = FeatureAlignmentLoss()
        
        # WandB ì´ˆê¸°í™”
        if self.use_wandb:
            self.init_wandb()
        
        # ëª¨ë¸ ì´ˆê¸° ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        self._test_model_outputs()
    
    def _select_device(self, device: str) -> str:
        """ë””ë°”ì´ìŠ¤ ìë™ ì„ íƒ"""
        if device == 'auto':
            # 1. CUDA ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
            if torch.cuda.is_available():
                device = 'cuda'
                gpu_count = torch.cuda.device_count()
                gpu_name = torch.cuda.get_device_name(0) if gpu_count > 0 else "Unknown"
                print(f"ğŸš€ CUDA ì‚¬ìš©: {gpu_name} ({gpu_count}ê°œ GPU)")
            # 2. Apple Silicon MPS ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ (macOS)
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                device = 'mps'
                print(f"ğŸ Apple Silicon MPS ì‚¬ìš©")
            # 3. ê¸°ë³¸ê°’ì€ CPU
            else:
                device = 'cpu'
                print(f"ğŸ’» CPU ì‚¬ìš©")
        else:
            # ìˆ˜ë™ ì§€ì •ëœ ë””ë°”ì´ìŠ¤ ìœ íš¨ì„± ê²€ì¦
            if device == 'cuda' and not torch.cuda.is_available():
                print(f"âš ï¸ CUDAê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. CPUë¡œ ë³€ê²½í•©ë‹ˆë‹¤.")
                device = 'cpu'
            elif device == 'mps' and not (hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()):
                print(f"âš ï¸ MPSê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. CPUë¡œ ë³€ê²½í•©ë‹ˆë‹¤.")
                device = 'cpu'
            else:
                print(f"ğŸ”§ ìˆ˜ë™ ì„ íƒëœ ë””ë°”ì´ìŠ¤: {device}")
        
        return device
    
    def _get_optimal_settings(self) -> dict:
        """ë””ë°”ì´ìŠ¤ë³„ ìµœì í™” ì„¤ì • ë°˜í™˜"""
        if self.device == 'cuda':
            # CUDA í™˜ê²½ - ë†’ì€ ì„±ëŠ¥ ì„¤ì •
            return {
                'batch_size_multiplier': 1.0,
                'num_workers_multiplier': 1.0,
                'pin_memory': True,
                'non_blocking': True
            }
        elif self.device == 'mps':
            # Apple Silicon MPS - ì¤‘ê°„ ì„±ëŠ¥ ì„¤ì •  
            return {
                'batch_size_multiplier': 0.75,
                'num_workers_multiplier': 0.5,
                'pin_memory': False,
                'non_blocking': False
            }
        else:
            # CPU - ì €ì„±ëŠ¥ ì„¤ì •
            return {
                'batch_size_multiplier': 0.5,
                'num_workers_multiplier': 0.25,
                'pin_memory': False,
                'non_blocking': False
            }
    
    def load_dataset_config(self, data_yaml: str):
        """ë°ì´í„°ì…‹ ì„¤ì • ë¡œë“œ ë° ìˆ˜ì •"""
        if not os.path.exists(data_yaml):
            print(f"âš ï¸ ë°ì´í„° ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_yaml}")
            self.data_yaml = data_yaml
            self.modified_data_yaml = data_yaml
            return
        
        # ì›ë³¸ ì„¤ì • ë¡œë“œ
        with open(data_yaml, 'r') as f:
            self.data_config = yaml.safe_load(f)
        
        # single-class ì„¤ì •ìœ¼ë¡œ ìˆ˜ì •
        self.data_config['nc'] = 1
        self.data_config['names'] = {0: 'ui_component'}
        
        if 'channels' not in self.data_config:
            self.data_config['channels'] = 3
        if 'imgsz' not in self.data_config:
            self.data_config['imgsz'] = 640
        
        # ìˆ˜ì •ëœ ì„¤ì • ì €ì¥
        modified_path = data_yaml.replace('.yaml', '_modified.yaml')
        with open(modified_path, 'w') as f:
            yaml.dump(self.data_config, f)
        
        self.data_yaml = data_yaml
        self.modified_data_yaml = modified_path
    
    def init_wandb(self):
        """WandB ì´ˆê¸°í™”"""
        try:
            current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
            wandb.init(
                project="figma-ui-yolo11-kd",
                name=f"distill_{current_time}",
                config={
                    'teacher': 'yolov11l',
                    'student': 'yolov11s',
                    'task': 'figma_ui_detection',
                    'method': 'knowledge_distillation'
                }
            )
        except Exception as e:
            print(f"âš ï¸ WandB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.use_wandb = False
    
    def _test_model_outputs(self):
        """ëª¨ë¸ ì¶œë ¥ í˜•ì‹ í…ŒìŠ¤íŠ¸"""
        print(f"\nğŸ§ª ëª¨ë¸ ì¶œë ¥ í˜•ì‹ í…ŒìŠ¤íŠ¸...")
        try:
            # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„± (640x640)
            test_img = torch.randn(1, 3, 640, 640).to(self.device)
            
            with torch.no_grad():
                # Teacher ì¶œë ¥ í…ŒìŠ¤íŠ¸
                teacher_out = self.teacher.model(test_img)
                print(f"ğŸ“ Teacher ì¶œë ¥:")
                print(f"   - íƒ€ì…: {type(teacher_out)}")
                if hasattr(teacher_out, 'shape'):
                    print(f"   - í˜•íƒœ: {teacher_out.shape}")
                elif hasattr(teacher_out, '__len__'):
                    print(f"   - ê¸¸ì´: {len(teacher_out)}")
                    if len(teacher_out) > 0:
                        print(f"   - ì²«ë²ˆì§¸ ìš”ì†Œ í˜•íƒœ: {getattr(teacher_out[0], 'shape', 'N/A')}")
                
                # Student ì¶œë ¥ í…ŒìŠ¤íŠ¸
                student_out = self.student.model(test_img)
                print(f"ğŸ’ Student ì¶œë ¥:")
                print(f"   - íƒ€ì…: {type(student_out)}")
                if hasattr(student_out, 'shape'):
                    print(f"   - í˜•íƒœ: {student_out.shape}")
                elif hasattr(student_out, '__len__'):
                    print(f"   - ê¸¸ì´: {len(student_out)}")
                    if len(student_out) > 0:
                        print(f"   - ì²«ë²ˆì§¸ ìš”ì†Œ í˜•íƒœ: {getattr(student_out[0], 'shape', 'N/A')}")
                
                # Student ì´ˆê¸° ì‹ ë¢°ë„ í™•ì¸
                if isinstance(student_out, torch.Tensor) and student_out.dim() == 3:
                    if student_out.shape[-1] >= 5:
                        conf_scores = torch.sigmoid(student_out[0, :, 4])
                        print(f"ğŸ” Student ì´ˆê¸° ì‹ ë¢°ë„:")
                        print(f"   - ìµœëŒ€: {conf_scores.max():.6f}")
                        print(f"   - í‰ê· : {conf_scores.mean():.6f}")
                        print(f"   - > 0.1: {(conf_scores > 0.1).sum().item()}ê°œ")
                        print(f"   - > 0.01: {(conf_scores > 0.01).sum().item()}ê°œ")
                        print(f"   - > 0.001: {(conf_scores > 0.001).sum().item()}ê°œ")
                
        except Exception as e:
            print(f"âš ï¸ ëª¨ë¸ ì¶œë ¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    def train_step(self, batch, optimizer) -> Dict:
        """ë‹¨ì¼ í•™ìŠµ ìŠ¤í… (ë¦¬íŒ©í† ë§)"""
        # ë°°ì¹˜ íŒŒì‹±
        images, targets = self._parse_batch(batch)
        if images is None:
            return {}
        
        # ëª¨ë¸ ì¶”ë¡ 
        try:
            teacher_outputs = self._get_teacher_predictions(images)
            student_outputs, raw_student_preds = self._get_student_predictions(images)
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ì¶”ë¡  ì¤‘ ì˜¤ë¥˜: {e}")
            return {}
        
        # ì†ì‹¤ ê³„ì‚°
        try:
            # 1. Detection ì¦ë¥˜ ì†ì‹¤
            det_loss, det_metrics = self.distillation_loss(
                student_outputs, teacher_outputs, targets
            )
            
            # 2. Feature ì¦ë¥˜ ì†ì‹¤ (í˜„ì¬ ë¹„í™œì„±í™”)
            feat_loss = torch.tensor(0.0, device=images.device, requires_grad=True)
            feat_metrics = {'feature_total': 0.0}
            
            # 3. Base ì†ì‹¤ (í˜„ì¬ ë¹„í™œì„±í™”)
            base_loss = torch.tensor(0.0, device=images.device, requires_grad=True)
            
            # ì „ì²´ ì†ì‹¤ ì¡°í•©
            total_loss = det_loss + 0.5 * feat_loss + 0.0 * base_loss
            
            # ì—­ì „íŒŒ
            optimizer.zero_grad()
            total_loss.backward()
            optimizer.step()
            
            # ë©”íŠ¸ë¦­ ë°˜í™˜
            return {
                'loss/total': total_loss.item(),
                'loss/detection': det_loss.item(),
                'loss/feature': feat_loss.item(),
                'loss/base': base_loss.item(),
                **det_metrics,
                **feat_metrics
            }
            
        except Exception as e:
            print(f"âŒ ì†ì‹¤ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
            return {}
    
    def _parse_batch(self, batch):
        """ë°°ì¹˜ ë°ì´í„° íŒŒì‹±"""
        if isinstance(batch, dict):
            images = batch.get('img', batch.get('image'))
            targets = batch
        elif isinstance(batch, (tuple, list)) and len(batch) >= 2:
            images, targets = batch[0], batch[1]
        else:
            return None, None
        
        if images is None:
            return None, None
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        images = images.float() / 255.0 if images.dtype == torch.uint8 else images
        images = images.to(self.device)
        
        return images, targets
    
    def _get_teacher_predictions(self, images):
        """Teacher ëª¨ë¸ ì˜ˆì¸¡"""
        # Teacher ëª¨ë¸ì˜ ì§ì ‘ ì¶”ë¡  ì‚¬ìš© (í›„ì²˜ë¦¬ í¬í•¨)
        results = self.teacher(images, verbose=False)
        return self._parse_yolo_results(results, 'teacher')
    
    def _get_student_predictions(self, images):
        """Student ëª¨ë¸ ì˜ˆì¸¡"""
        # Student ëª¨ë¸ì˜ ì§ì ‘ ì¶”ë¡  ì‚¬ìš© (í›„ì²˜ë¦¬ í¬í•¨)
        results = self.student(images, verbose=False)
        
        # YOLO Results ê°ì²´ë¥¼ ì§ì ‘ íŒŒì‹±
        parsed_outputs = self._parse_yolo_results(results, 'student')
        
        # Raw ì¶œë ¥ë„ í•¨ê»˜ ë°˜í™˜ (ì†ì‹¤ ê³„ì‚°ìš©)
        raw_student_preds = self.student.model(images)
        return parsed_outputs, raw_student_preds
    
    def _parse_yolo_results(self, results, model_name):
        """YOLO Results ê°ì²´ë¥¼ íŒŒì‹±"""
        try:
            batch_size = len(results) if isinstance(results, list) else 1
            if not isinstance(results, list):
                results = [results]
            
            all_bbox = []
            all_objectness = []
            
            for i, result in enumerate(results):
                if hasattr(result, 'boxes') and result.boxes is not None:
                    # ê²€ì¶œëœ ê°ì²´ê°€ ìˆëŠ” ê²½ìš°
                    boxes = result.boxes
                    if hasattr(boxes, 'xyxy'):
                        bbox = boxes.xyxy.cpu().numpy()  # [N, 4] xyxy format
                        conf = boxes.conf.cpu().numpy()  # [N] confidence scores
                        
                        print(f"âœ… {model_name} ì´ë¯¸ì§€ {i}: {len(bbox)}ê°œ ê²€ì¶œ (í‰ê·  ì‹ ë¢°ë„: {conf.mean():.4f})")
                        
                        all_bbox.append(torch.tensor(bbox, device=self.device))
                        all_objectness.append(torch.tensor(conf, device=self.device).unsqueeze(-1))
                    else:
                        print(f"âš ï¸ {model_name} ì´ë¯¸ì§€ {i}: boxes.xyxy ì—†ìŒ")
                        all_bbox.append(torch.empty((0, 4), device=self.device))
                        all_objectness.append(torch.empty((0, 1), device=self.device))
                else:
                    print(f"âš ï¸ {model_name} ì´ë¯¸ì§€ {i}: ê²€ì¶œ ì—†ìŒ")
                    all_bbox.append(torch.empty((0, 4), device=self.device))
                    all_objectness.append(torch.empty((0, 1), device=self.device))
            
            # ë°°ì¹˜ ì°¨ì›ìœ¼ë¡œ ê²°í•©
            max_detections = max(bbox.shape[0] for bbox in all_bbox) if all_bbox else 0
            if max_detections == 0:
                return {
                    'bbox': torch.empty((batch_size, 0, 4), device=self.device),
                    'objectness': torch.empty((batch_size, 0, 1), device=self.device)
                }
            
            # íŒ¨ë”©í•˜ì—¬ ë™ì¼í•œ í¬ê¸°ë¡œ ë§Œë“¤ê¸°
            padded_bbox = []
            padded_objectness = []
            
            for bbox, obj in zip(all_bbox, all_objectness):
                if bbox.shape[0] < max_detections:
                    pad_size = max_detections - bbox.shape[0]
                    bbox_pad = torch.zeros((pad_size, 4), device=self.device)
                    obj_pad = torch.zeros((pad_size, 1), device=self.device)
                    bbox = torch.cat([bbox, bbox_pad], dim=0)
                    obj = torch.cat([obj, obj_pad], dim=0)
                
                padded_bbox.append(bbox)
                padded_objectness.append(obj)
            
            return {
                'bbox': torch.stack(padded_bbox, dim=0),
                'objectness': torch.stack(padded_objectness, dim=0)
            }
            
        except Exception as e:
            print(f"âŒ {model_name} Results íŒŒì‹± ì˜¤ë¥˜: {e}")
            batch_size = len(results) if isinstance(results, list) else 1
            return {
                'bbox': torch.empty((batch_size, 0, 4), device=self.device),
                'objectness': torch.empty((batch_size, 0, 1), device=self.device)
            }
    
    def parse_model_outputs(self, outputs):
        """ëª¨ë¸ ì¶œë ¥ì„ íŒŒì‹±í•˜ì—¬ bboxì™€ objectness ë¶„ë¦¬"""
        if outputs.dim() == 3:  # [batch, num_anchors, features]
            if outputs.shape[-1] >= 5:  # [x, y, w, h, conf, ...]
                return {
                    'bbox': outputs[..., :4],
                    'objectness': outputs[..., 4:5]
                }
            else:
                batch_size = outputs.shape[0]
                return {
                    'bbox': torch.zeros(batch_size, 1, 4, device=outputs.device),
                    'objectness': torch.zeros(batch_size, 1, 1, device=outputs.device)
                }
        elif outputs.dim() == 4:  # [batch, features, height, width]
            batch_size, features, h, w = outputs.shape
            outputs_flat = outputs.view(batch_size, features, h * w).transpose(1, 2)
            
            if features >= 5:
                return {
                    'bbox': outputs_flat[..., :4],
                    'objectness': outputs_flat[..., 4:5]
                }
            else:
                return {
                    'bbox': torch.zeros(batch_size, h*w, 4, device=outputs.device),
                    'objectness': torch.zeros(batch_size, h*w, 1, device=outputs.device)
                }
        else:
            batch_size = outputs.shape[0] if outputs.dim() > 0 else 1
            return {
                'bbox': torch.zeros(batch_size, 1, 4, device=outputs.device),
                'objectness': torch.zeros(batch_size, 1, 1, device=outputs.device)
            }
    
    def validate(self, val_loader):
        """ê²€ì¦ ìˆ˜í–‰ - bbox ì¶œë ¥ ë° ì‹¤ì œ mAP ê³„ì‚°"""
        if val_loader is None:
            print("âš ï¸ Validation loaderê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {
                'val/mAP': 0.0,
                'val/precision': 0.0,
                'val/recall': 0.0,
                'val/inference_time': 0.0
            }
        
        print(f"\nğŸ” Validation ì‹œì‘...")
        self.student.model.eval()
        
        try:
            # ìƒì„¸í•œ í‰ê°€ ìˆ˜í–‰
            eval_metrics = self.evaluate_model(val_loader, epoch=0)
            
            # ì¶”ë¡  ì‹œê°„ ì¸¡ì •
            print(f"â±ï¸ ì¶”ë¡  ì‹œê°„ ì¸¡ì • ì¤‘...")
            total_time = 0
            num_batches = 0
            
            with torch.no_grad():
                for batch_idx, batch in enumerate(val_loader):
                    if batch_idx >= 5:  # ì ì€ ìˆ˜ë¡œ ë¹ ë¥¸ ì¸¡ì •
                        break
                        
                    images, _ = self._parse_batch(batch)
                    if images is None:
                        continue
                    
                    start_time = time.time()
                    results = self.student.model(images)
                    inference_time = time.time() - start_time
                    total_time += inference_time
                    num_batches += 1
                    
                    # ì²« ë²ˆì§¸ ë°°ì¹˜ì—ì„œ ìƒì„¸ ì¶œë ¥
                    if batch_idx == 0:
                        print(f"ğŸ’¡ ë°°ì¹˜ {batch_idx}: {inference_time*1000:.2f}ms, ì´ë¯¸ì§€ {images.shape[0]}ê°œ")
                        if hasattr(results, '__iter__') and not isinstance(results, torch.Tensor):
                            for i, result in enumerate(results):
                                if hasattr(result, 'boxes') and result.boxes is not None:
                                    num_detections = len(result.boxes)
                                    print(f"   ğŸ“¦ ì´ë¯¸ì§€ {i}: {num_detections}ê°œ ê²€ì¶œ")
            
            avg_time = total_time / num_batches if num_batches > 0 else 0
            
            print(f"âœ… Validation ì™„ë£Œ!")
            print(f"   ğŸ¯ mAP@0.5: {eval_metrics.get('map50', 0.0):.4f}")
            print(f"   ğŸ¯ mAP@0.5:0.95: {eval_metrics.get('map', 0.0):.4f}")
            print(f"   ğŸ” Precision: {eval_metrics.get('precision', 0.0):.4f}")
            print(f"   ğŸ” Recall: {eval_metrics.get('recall', 0.0):.4f}")
            print(f"   â±ï¸ í‰ê·  ì¶”ë¡  ì‹œê°„: {avg_time*1000:.2f}ms")
            
            # WandBì— bbox ì´ë¯¸ì§€ ë¡œê¹…
            if self.use_wandb:
                self.log_inference_images(val_loader, epoch=0, num_images=3)
            
            self.student.model.train()
            return {
                'val/mAP': eval_metrics.get('map50', 0.0),
                'val/mAP_50_95': eval_metrics.get('map', 0.0),
                'val/precision': eval_metrics.get('precision', 0.0),
                'val/recall': eval_metrics.get('recall', 0.0),
                'val/inference_time': avg_time
            }
            
        except Exception as e:
            print(f"âŒ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            self.student.model.train()
            return {
                'val/mAP': 0.0,
                'val/precision': 0.0,
                'val/recall': 0.0,
                'val/inference_time': 0.0
            }
    
    def train(self,
              epochs: int = 100,
              batch_size: int = 16,
              learning_rate: float = 0.001,
              num_workers: int = 2,
              save_dir: str = './runs',
              max_train_samples: int = None,
              max_val_samples: int = None):
        """Knowledge Distillation í•™ìŠµ ì‹¤í–‰"""
        
        # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(save_dir, exist_ok=True)
        
        # ë°ì´í„°ë¡œë” ìƒì„±
        train_loader, val_loader = self._create_dataloaders(
            batch_size, num_workers, max_train_samples, max_val_samples
        )
        
        if train_loader is None:
            print("âš ï¸ ë°ì´í„°ë¡œë” ìƒì„± ì‹¤íŒ¨ - YOLO ê¸°ë³¸ í•™ìŠµìœ¼ë¡œ í´ë°±")
            return self._train_with_yolo_default(epochs, batch_size, learning_rate, save_dir)
        
        # ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
        optimizer = torch.optim.AdamW(self.student.model.parameters(), lr=learning_rate)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
        
        best_map = 0.0
        
        print(f"ğŸ¯ Knowledge Distillation í•™ìŠµ ì‹œì‘: {epochs} epochs")
        print("-" * 50)
        
        for epoch in range(epochs):
            print(f"\n[Epoch {epoch+1}/{epochs}]")
            
            # í•™ìŠµ
            epoch_metrics = {}
            for batch_idx, batch in enumerate(train_loader):
                try:
                    metrics = self.train_step(batch, optimizer)
                    
                    # ë°°ì¹˜ ë©”íŠ¸ë¦­ ëˆ„ì 
                    for k, v in metrics.items():
                        if k not in epoch_metrics:
                            epoch_metrics[k] = 0
                        epoch_metrics[k] += v
                    
                    # ì£¼ê¸°ì  ë¡œê¹… (ê°„ê²°í•˜ê²Œ)
                    if batch_idx % 20 == 0:
                        print(f"  ğŸ“Š Batch {batch_idx}: Loss = {metrics.get('loss/total', 0):.4f}")
                        if self.use_wandb:
                            wandb.log(metrics, step=epoch * len(train_loader) + batch_idx)
                            
                except Exception as batch_error:
                    print(f"âŒ Batch {batch_idx} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {batch_error}")
                    if batch_idx == 0:
                        print("ì²« ë²ˆì§¸ ë°°ì¹˜ë¶€í„° ì˜¤ë¥˜ ë°œìƒ. í•™ìŠµì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                        return 0
                    continue
            
            # ê²€ì¦ ë° í‰ê°€
            val_metrics = self.validate(val_loader)
            
            # mAP í‰ê°€ (ë§¤ 5 ì—í­ë§ˆë‹¤)
            if val_loader is not None and (epoch + 1) % 5 == 0:
                try:
                    eval_metrics = self.evaluate_model(val_loader, epoch + 1)
                    val_metrics.update({f'eval_{k}': v for k, v in eval_metrics.items()})
                    
                    # ì¶”ë¡  ì´ë¯¸ì§€ ë¡œê¹… (ë§¤ 10 ì—í­ë§ˆë‹¤)
                    if (epoch + 1) % 10 == 0:
                        self.log_inference_images(val_loader, epoch + 1, num_images=3)
                        
                except Exception as eval_error:
                    print(f"âš ï¸ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {eval_error}")
            
            # ì—í­ í‰ê·  ë©”íŠ¸ë¦­
            for k in epoch_metrics:
                epoch_metrics[k] /= len(train_loader)
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
            scheduler.step()
            
            # ì—í­ ê²°ê³¼ í‘œì‹œ
            print(f"ğŸ“ˆ Epoch {epoch+1}/{epochs} ì™„ë£Œ:")
            print(f"   ğŸ”¥ í‰ê·  ì†ì‹¤: {epoch_metrics['loss/total']:.4f}")
            print(f"   ğŸ¯ mAP@50: {val_metrics.get('val/mAP', 0):.4f}")
            print(f"   âš¡ ì¶”ë¡  ì‹œê°„: {val_metrics.get('val/inference_time', 0)*1000:.1f}ms")
            
            # WandB ë¡œê¹…
            if self.use_wandb:
                wandb.log({
                    'epoch': epoch + 1,
                    **epoch_metrics,
                    **val_metrics,
                    'lr': scheduler.get_last_lr()[0]
                })
            
            # ëª¨ë¸ ì €ì¥
            if val_metrics['val/mAP'] > best_map:
                best_map = val_metrics['val/mAP']
                save_path = f"{save_dir}/best_model.pt"
                torch.save(self.student.model.state_dict(), save_path)
                print(f"ğŸŒŸ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! mAP: {best_map:.4f}")
                
                if self.use_wandb:
                    wandb.save(save_path)
            
            print("-" * 40)
        
        return best_map
    
    def _create_dataloaders(self, batch_size, num_workers, max_train_samples=None, max_val_samples=None):
        """ë°ì´í„°ë¡œë” ìƒì„± (ë””ë°”ì´ìŠ¤ë³„ ìµœì í™”)"""
        try:
            from ultralytics.data.dataset import YOLODataset
            from torch.utils.data import DataLoader
            import yaml
            
            # ë””ë°”ì´ìŠ¤ë³„ ìµœì í™” ì„¤ì • ì ìš©
            optimal_settings = self._get_optimal_settings()
            
            # ë°°ì¹˜ í¬ê¸°ì™€ ì›Œì»¤ ìˆ˜ ì¡°ì •
            optimized_batch_size = max(1, int(batch_size * optimal_settings['batch_size_multiplier']))
            optimized_num_workers = max(1, int(num_workers * optimal_settings['num_workers_multiplier']))
            
            print(f"ğŸ”§ ë””ë°”ì´ìŠ¤ë³„ ìµœì í™” ì„¤ì •:")
            print(f"   - ë°°ì¹˜ í¬ê¸°: {batch_size} â†’ {optimized_batch_size}")
            print(f"   - ì›Œì»¤ ìˆ˜: {num_workers} â†’ {optimized_num_workers}")
            print(f"   - Pin Memory: {optimal_settings['pin_memory']}")
            
            batch_size = optimized_batch_size
            num_workers = optimized_num_workers
            
            with open(self.modified_data_yaml, 'r') as f:
                data_config = yaml.safe_load(f)
            
            data_path = data_config.get('path', './datasets/figma_ui')
            train_path = os.path.join(data_path, data_config.get('train', 'images/train'))
            val_path = os.path.join(data_path, data_config.get('val', 'images/val'))
            
            if not os.path.exists(train_path):
                print(f"âŒ í•™ìŠµ ë°ì´í„° ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {train_path}")
                return None, None
            
            # ë°ì´í„°ì…‹ ìƒì„±
            train_dataset = YOLODataset(
                img_path=train_path,
                data=data_config,
                task='detect',
                imgsz=640,
                batch_size=batch_size,
                augment=True,
                cache=False,
                single_cls=True,
                stride=32
            )
            
            # ìƒ˜í”Œ ìˆ˜ ì œí•œ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)
            if max_train_samples and len(train_dataset) > max_train_samples:
                train_dataset.im_files = train_dataset.im_files[:max_train_samples]
                train_dataset.labels = train_dataset.labels[:max_train_samples]
                print(f"ğŸ”¸ í•™ìŠµ ë°ì´í„°ë¥¼ {len(train_dataset.im_files)}ê°œë¡œ ì œí•œ")
            
            train_loader = DataLoader(
                train_dataset,
                batch_size=batch_size,
                shuffle=True,
                num_workers=num_workers,
                pin_memory=optimal_settings['pin_memory'],
                drop_last=True,
                collate_fn=train_dataset.collate_fn
            )
            
            val_loader = None
            if os.path.exists(val_path):
                try:
                    val_dataset = YOLODataset(
                        img_path=val_path,
                        data=data_config,
                        task='detect',
                        imgsz=640,
                        batch_size=batch_size,
                        augment=False,
                        cache=False,
                        single_cls=True,
                        stride=32
                    )
                    
                    # ê²€ì¦ ìƒ˜í”Œ ìˆ˜ ì œí•œ
                    if max_val_samples and len(val_dataset) > max_val_samples:
                        val_dataset.im_files = val_dataset.im_files[:max_val_samples]
                        val_dataset.labels = val_dataset.labels[:max_val_samples]
                        print(f"ğŸ”¸ ê²€ì¦ ë°ì´í„°ë¥¼ {len(val_dataset.im_files)}ê°œë¡œ ì œí•œ")
                    
                    val_loader = DataLoader(
                        val_dataset,
                        batch_size=batch_size,
                        shuffle=False,
                        num_workers=num_workers,
                        pin_memory=optimal_settings['pin_memory'],
                        drop_last=False,
                        collate_fn=val_dataset.collate_fn
                    )
                    
                except Exception as val_e:
                    print(f"âš ï¸ ê²€ì¦ ë°ì´í„°ë¡œë” ìƒì„± ì‹¤íŒ¨: {val_e}")
            
            print(f"âœ… ë°ì´í„°ë¡œë” ìƒì„± ì™„ë£Œ!")
            print(f"   í•™ìŠµ: {len(train_loader)} batches")
            if val_loader:
                print(f"   ê²€ì¦: {len(val_loader)} batches")
            
            return train_loader, val_loader
            
        except Exception as e:
            print(f"âŒ ë°ì´í„°ë¡œë” ìƒì„± ì‹¤íŒ¨: {e}")
            return None, None
    
    def _train_with_yolo_default(self, epochs, batch_size, learning_rate, save_dir):
        """YOLO ê¸°ë³¸ í•™ìŠµ ë°©ì‹ í´ë°±"""
        print("ğŸ”„ YOLO ê¸°ë³¸ í•™ìŠµ ë°©ì‹ìœ¼ë¡œ ì „í™˜...")
        
        try:
            results = self.student.train(
                data=self.modified_data_yaml,
                epochs=epochs,
                batch=batch_size,
                lr0=learning_rate,
                device=self.device,
                project=save_dir,
                name='student_model',
                exist_ok=True,
                verbose=True
            )
            return float(results.results_dict.get('metrics/mAP50(B)', 0))
        except Exception as e:
            print(f"âŒ YOLO ê¸°ë³¸ í•™ìŠµë„ ì‹¤íŒ¨: {e}")
            return 0.0
    
    # í‰ê°€ ë° ì‹œê°í™” ë©”ì„œë“œë“¤ì€ ê¸°ì¡´ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€
    def evaluate_model(self, val_loader, epoch: int) -> Dict:
        """ëª¨ë¸ í‰ê°€ ë° mAP ê³„ì‚°"""
        self.student.model.eval()
        
        all_predictions = []
        all_targets = []
        
        print(f"\nğŸ“Š Epoch {epoch} ëª¨ë¸ í‰ê°€ ì‹œì‘...")
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(val_loader):
                if batch_idx >= 20:  # ë¹ ë¥¸ í‰ê°€ë¥¼ ìœ„í•´ 20ë°°ì¹˜ë§Œ ì‚¬ìš©
                    break
                    
                # ë°°ì¹˜ íŒŒì‹±
                images, targets = self._parse_batch(batch)
                if images is None:
                    continue
                
                # Student ì¶”ë¡ 
                try:
                    results = self.student.model(images)
                    
                    # ë””ë²„ê¹…: ëª¨ë¸ ì¶œë ¥ í˜•íƒœ í™•ì¸
                    print(f"ğŸ” ëª¨ë¸ ì¶œë ¥ íƒ€ì…: {type(results)}")
                    if hasattr(results, '__len__'):
                        print(f"ğŸ” ì¶œë ¥ ê¸¸ì´: {len(results)}")
                    
                    # YOLO ê²°ê³¼ë¥¼ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    if hasattr(results, '__iter__') and not isinstance(results, torch.Tensor):
                        # Ultralytics YOLO ê²°ê³¼
                        for i, result in enumerate(results):
                            print(f"ğŸ” Result {i} íƒ€ì…: {type(result)}")
                            print(f"ğŸ” Result {i} ì†ì„±: {dir(result)}")
                            
                            if hasattr(result, 'boxes') and result.boxes is not None:
                                boxes = result.boxes
                                print(f"ğŸ” Boxes íƒ€ì…: {type(boxes)}, ê¸¸ì´: {len(boxes)}")
                                
                                if len(boxes) > 0:
                                    pred_boxes = boxes.xyxy.cpu().numpy()
                                    pred_scores = boxes.conf.cpu().numpy()
                                    pred_labels = boxes.cls.cpu().numpy()
                                    
                                    # bbox ì¶œë ¥ ë¡œê¹…
                                    print(f"ğŸ¯ ì´ë¯¸ì§€ {batch_idx}_{i}: {len(pred_boxes)}ê°œ ê°ì²´ ê²€ì¶œ")
                                    for j in range(min(3, len(pred_boxes))):  # ìƒìœ„ 3ê°œë§Œ ì¶œë ¥
                                        bbox = pred_boxes[j]
                                        print(f"   - Box {j}: [{bbox[0]:.1f}, {bbox[1]:.1f}, {bbox[2]:.1f}, {bbox[3]:.1f}] conf:{pred_scores[j]:.3f}")
                                    
                                    for j in range(len(pred_boxes)):
                                        all_predictions.append({
                                            'image_id': batch_idx * images.shape[0] + i,
                                            'bbox': pred_boxes[j],
                                            'score': pred_scores[j],
                                            'label': pred_labels[j]
                                        })
                                else:
                                    print(f"âš ï¸ ì´ë¯¸ì§€ {batch_idx}_{i}: ê²€ì¶œëœ ê°ì²´ ì—†ìŒ")
                            else:
                                print(f"âš ï¸ ì´ë¯¸ì§€ {batch_idx}_{i}: boxes ì†ì„± ì—†ìŒ ë˜ëŠ” None")
                    else:
                        # Raw tensor ì¶œë ¥ ì²˜ë¦¬
                        print(f"âš ï¸ Raw tensor ì¶œë ¥ ê°ì§€:")
                        print(f"   - íƒ€ì…: {type(results)}")
                        if isinstance(results, torch.Tensor):
                            print(f"   - í˜•íƒœ: {results.shape}")
                            print(f"   - ê°’ ë²”ìœ„: [{results.min():.3f}, {results.max():.3f}]")
                            
                            # YOLOv11 ì¶œë ¥ ì²˜ë¦¬ ê°œì„ 
                            # ì¼ë°˜ì ì¸ YOLO ì¶œë ¥ í˜•ì‹ë“¤ì„ ì²˜ë¦¬
                            if results.dim() == 3:
                                batch_size = results.shape[0]
                                
                                # í˜•ì‹ 1: [batch, num_anchors, features] - ì£¼ìš” ì¶œë ¥ í˜•ì‹
                                if results.shape[-1] >= 5:  # [cx, cy, w, h, conf, ...] or [x1, y1, x2, y2, conf, ...]
                                    for i in range(batch_size):
                                        detections = results[i]  # [num_anchors, features]
                                        
                                        # objectness/confidence ì ìˆ˜ ì¶”ì¶œ
                                        if results.shape[-1] == 5:  # [x, y, w, h, conf]
                                            conf_scores = torch.sigmoid(detections[:, 4])
                                        elif results.shape[-1] == 6:  # [x1, y1, x2, y2, conf, cls]
                                            conf_scores = torch.sigmoid(detections[:, 4])
                                        else:  # ë” ë§ì€ íŠ¹ì„±ì´ ìˆëŠ” ê²½ìš°
                                            conf_scores = torch.sigmoid(detections[:, 4])
                                        
                                        # ë§¤ìš° ë‚®ì€ ì„ê³„ê°’ìœ¼ë¡œ í•„í„°ë§
                                        conf_mask = conf_scores > 0.001  # ë” ë‚®ì€ ì„ê³„ê°’
                                        
                                        if conf_mask.any():
                                            valid_detections = detections[conf_mask]
                                            valid_scores = conf_scores[conf_mask]
                                            
                                            # bbox ì¢Œí‘œ ì²˜ë¦¬
                                            if results.shape[-1] >= 4:
                                                bbox_coords = valid_detections[:, :4].cpu().numpy()
                                                scores = valid_scores.cpu().numpy()
                                                
                                                # bboxê°€ xywh í˜•ì‹ì¸ì§€ xyxy í˜•ì‹ì¸ì§€ í™•ì¸
                                                # ì¼ë°˜ì ìœ¼ë¡œ YOLOëŠ” center_x, center_y, width, height í˜•ì‹
                                                # ì´ë¥¼ x1, y1, x2, y2ë¡œ ë³€í™˜
                                                h, w = images.shape[2], images.shape[3]  # ì´ë¯¸ì§€ í¬ê¸°
                                                
                                                converted_boxes = []
                                                for bbox in bbox_coords:
                                                    cx, cy, bw, bh = bbox
                                                    # ì •ê·œí™”ëœ ì¢Œí‘œë¥¼ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜
                                                    cx *= w
                                                    cy *= h
                                                    bw *= w
                                                    bh *= h
                                                    
                                                    x1 = max(0, cx - bw/2)
                                                    y1 = max(0, cy - bh/2)
                                                    x2 = min(w, cx + bw/2)
                                                    y2 = min(h, cy + bh/2)
                                                    converted_boxes.append([x1, y1, x2, y2])
                                                
                                                converted_boxes = np.array(converted_boxes)
                                                pred_labels = np.zeros(len(converted_boxes))  # single class
                                                
                                                print(f"ğŸ¯ Raw tensor ì´ë¯¸ì§€ {batch_idx}_{i}: {len(converted_boxes)}ê°œ ê°ì²´ ê²€ì¶œ (ì„ê³„ê°’ 0.001)")
                                                for j in range(min(3, len(converted_boxes))):
                                                    bbox = converted_boxes[j]
                                                    print(f"   - Box {j}: [{bbox[0]:.1f}, {bbox[1]:.1f}, {bbox[2]:.1f}, {bbox[3]:.1f}] conf:{scores[j]:.4f}")
                                                
                                                for j in range(len(converted_boxes)):
                                                    all_predictions.append({
                                                        'image_id': batch_idx * images.shape[0] + i,
                                                        'bbox': converted_boxes[j],
                                                        'score': scores[j],
                                                        'label': pred_labels[j]
                                                    })
                                        else:
                                            print(f"âš ï¸ Raw tensor ì´ë¯¸ì§€ {batch_idx}_{i}: ì„ê³„ê°’ 0.001ë¡œë„ ê²€ì¶œ ì—†ìŒ")
                                            print(f"   - ìµœëŒ€ ì‹ ë¢°ë„: {conf_scores.max():.6f}")
                                            print(f"   - í‰ê·  ì‹ ë¢°ë„: {conf_scores.mean():.6f}")
                        elif isinstance(results, (list, tuple)):
                            print(f"   - ë¦¬ìŠ¤íŠ¸/íŠœí”Œ ê¸¸ì´: {len(results)}")
                            for i, item in enumerate(results):
                                print(f"   - Item {i}: {type(item)}, í˜•íƒœ: {getattr(item, 'shape', 'N/A')}")
                    
                    # Ground Truth ì²˜ë¦¬
                    if isinstance(targets, dict):
                        self._process_ground_truth(targets, images, batch_idx, all_targets)
                
                except Exception as e:
                    print(f"âš ï¸ í‰ê°€ ì¤‘ ì˜¤ë¥˜ (ë°°ì¹˜ {batch_idx}): {e}")
                    continue
        
        # mAP ê³„ì‚°
        metrics = self.calculate_map(all_predictions, all_targets)
        
        print(f"ğŸ“Š í‰ê°€ ì™„ë£Œ:")
        print(f"   - ì´ ì˜ˆì¸¡: {len(all_predictions)}ê°œ")
        print(f"   - ì´ GT: {len(all_targets)}ê°œ") 
        print(f"   - mAP@0.5: {metrics.get('map50', 0.0):.4f}")
        print(f"   - mAP@0.5:0.95: {metrics.get('map', 0.0):.4f}")
        
        self.student.model.train()
        return metrics
    
    def _process_ground_truth(self, targets, images, batch_idx, all_targets):
        """Ground Truth ë°ì´í„° ì²˜ë¦¬"""
        try:
            if 'bboxes' in targets:
                batch_idx_tensor = targets.get('batch_idx', torch.arange(images.shape[0]))
                bboxes = targets['bboxes']
                cls = targets.get('cls', torch.zeros(len(bboxes)))
                
                for img_idx in range(images.shape[0]):
                    mask = batch_idx_tensor == img_idx
                    if mask.any():
                        img_bboxes = bboxes[mask].cpu().numpy()
                        img_cls = cls[mask].cpu().numpy()
                        
                        print(f"ğŸ“‹ GT ì´ë¯¸ì§€ {batch_idx}_{img_idx}: {len(img_bboxes)}ê°œ ê°ì²´")
                        
                        for k in range(len(img_bboxes)):
                            # YOLO format (cx, cy, w, h) to (x1, y1, x2, y2)
                            bbox = img_bboxes[k]
                            h, w = images.shape[2], images.shape[3]
                            x1 = (bbox[0] - bbox[2]/2) * w
                            y1 = (bbox[1] - bbox[3]/2) * h
                            x2 = (bbox[0] + bbox[2]/2) * w
                            y2 = (bbox[1] + bbox[3]/2) * h
                            
                            all_targets.append({
                                'image_id': batch_idx * images.shape[0] + img_idx,
                                'bbox': [x1, y1, x2, y2],
                                'label': img_cls[k]
                            })
        except Exception as e:
            print(f"âš ï¸ GT ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def calculate_map(self, predictions, targets):
        """mAP ê³„ì‚°"""
        if not predictions or not targets:
            print("âš ï¸ ì˜ˆì¸¡ ë˜ëŠ” GTê°€ ì—†ì–´ì„œ mAPë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {'map': 0.0, 'map50': 0.0, 'precision': 0.0, 'recall': 0.0}
        
        import numpy as np
        
        # IoU ì„ê³„ê°’ë“¤
        iou_thresholds = np.arange(0.5, 1.0, 0.05)
        
        # ì´ë¯¸ì§€ë³„ë¡œ ê·¸ë£¹í™”
        pred_by_image = {}
        gt_by_image = {}
        
        for pred in predictions:
            img_id = pred['image_id']
            if img_id not in pred_by_image:
                pred_by_image[img_id] = []
            pred_by_image[img_id].append(pred)
        
        for gt in targets:
            img_id = gt['image_id']
            if img_id not in gt_by_image:
                gt_by_image[img_id] = []
            gt_by_image[img_id].append(gt)
        
        # ê° IoU ì„ê³„ê°’ì—ì„œ AP ê³„ì‚°
        aps = []
        for iou_thresh in iou_thresholds:
            ap = self.calculate_ap_at_iou(pred_by_image, gt_by_image, iou_thresh)
            aps.append(ap)
        
        # mAP@0.5
        map50 = self.calculate_ap_at_iou(pred_by_image, gt_by_image, 0.5)
        
        # mAP@0.5:0.95
        map_avg = np.mean(aps)
        
        # Precision, Recall at IoU=0.5
        precision, recall = self.calculate_precision_recall(pred_by_image, gt_by_image, 0.5)
        
        return {
            'map': map_avg,
            'map50': map50,
            'precision': precision,
            'recall': recall
        }
    
    def calculate_ap_at_iou(self, pred_by_image, gt_by_image, iou_thresh):
        """íŠ¹ì • IoU ì„ê³„ê°’ì—ì„œ AP ê³„ì‚°"""
        import numpy as np
        
        all_scores = []
        all_matches = []
        total_gt = 0
        
        for img_id in gt_by_image:
            gt_boxes = gt_by_image[img_id]
            pred_boxes = pred_by_image.get(img_id, [])
            
            total_gt += len(gt_boxes)
            
            if not pred_boxes:
                continue
            
            # ì˜ˆì¸¡ì„ ì‹ ë¢°ë„ ìˆœìœ¼ë¡œ ì •ë ¬
            pred_boxes = sorted(pred_boxes, key=lambda x: x['score'], reverse=True)
            
            matched_gt = [False] * len(gt_boxes)
            
            for pred in pred_boxes:
                all_scores.append(pred['score'])
                
                # ê°€ì¥ ë†’ì€ IoUë¥¼ ê°€ì§„ GT ì°¾ê¸°
                best_iou = 0
                best_gt_idx = -1
                
                for gt_idx, gt in enumerate(gt_boxes):
                    if matched_gt[gt_idx]:
                        continue
                    
                    iou = self.calculate_iou(pred['bbox'], gt['bbox'])
                    if iou > best_iou:
                        best_iou = iou
                        best_gt_idx = gt_idx
                
                # ë§¤ì¹­ í™•ì¸
                if best_iou >= iou_thresh and best_gt_idx >= 0:
                    matched_gt[best_gt_idx] = True
                    all_matches.append(True)
                else:
                    all_matches.append(False)
        
        if not all_scores:
            return 0.0
        
        # Precision-Recall ê³¡ì„  ê³„ì‚°
        all_scores = np.array(all_scores)
        all_matches = np.array(all_matches)
        
        # ì •ë ¬
        sort_idx = np.argsort(-all_scores)
        all_matches = all_matches[sort_idx]
        
        # ëˆ„ì  TP, FP ê³„ì‚°
        tp = np.cumsum(all_matches)
        fp = np.cumsum(~all_matches)
        
        # Precision, Recall ê³„ì‚°
        precision = tp / (tp + fp)
        recall = tp / total_gt if total_gt > 0 else np.zeros_like(tp)
        
        # AP ê³„ì‚° (11-point interpolation)
        ap = 0
        for t in np.arange(0, 1.1, 0.1):
            p_max = np.max(precision[recall >= t]) if np.any(recall >= t) else 0
            ap += p_max / 11
        
        return ap
    
    def calculate_precision_recall(self, pred_by_image, gt_by_image, iou_thresh=0.5):
        """Precisionê³¼ Recall ê³„ì‚°"""
        import numpy as np
        
        total_tp = 0
        total_fp = 0
        total_gt = 0
        
        for img_id in gt_by_image:
            gt_boxes = gt_by_image[img_id]
            pred_boxes = pred_by_image.get(img_id, [])
            
            total_gt += len(gt_boxes)
            
            if not pred_boxes:
                continue
            
            matched_gt = [False] * len(gt_boxes)
            
            for pred in pred_boxes:
                best_iou = 0
                best_gt_idx = -1
                
                for gt_idx, gt in enumerate(gt_boxes):
                    if matched_gt[gt_idx]:
                        continue
                    
                    iou = self.calculate_iou(pred['bbox'], gt['bbox'])
                    if iou > best_iou:
                        best_iou = iou
                        best_gt_idx = gt_idx
                
                if best_iou >= iou_thresh and best_gt_idx >= 0:
                    matched_gt[best_gt_idx] = True
                    total_tp += 1
                else:
                    total_fp += 1
        
        precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0
        recall = total_tp / total_gt if total_gt > 0 else 0
        
        return precision, recall
    
    def calculate_iou(self, box1, box2):
        """IoU ê³„ì‚°"""
        # box format: [x1, y1, x2, y2]
        x1 = max(box1[0], box2[0])
        y1 = max(box1[1], box2[1])
        x2 = min(box1[2], box2[2])
        y2 = min(box1[3], box2[3])
        
        if x2 <= x1 or y2 <= y1:
            return 0.0
        
        intersection = (x2 - x1) * (y2 - y1)
        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
        union = area1 + area2 - intersection
        
        return intersection / union if union > 0 else 0.0
    
    def log_inference_images(self, val_loader, epoch: int, num_images: int = 5):
        """ì¶”ë¡  ê²°ê³¼ ì´ë¯¸ì§€ë¥¼ bboxì™€ í•¨ê»˜ WandBì— ë¡œê¹…"""
        if not self.use_wandb:
            return
        
        print(f"ğŸ–¼ï¸ {num_images}ê°œ ì¶”ë¡  ì´ë¯¸ì§€ ë¡œê¹… (ì—í­ {epoch})")
        
        try:
            import wandb
            import matplotlib.pyplot as plt
            import matplotlib.patches as patches
            import numpy as np
            from PIL import Image
            
            self.student.model.eval()
            wandb_images = []
            
            with torch.no_grad():
                for batch_idx, batch in enumerate(val_loader):
                    if len(wandb_images) >= num_images:
                        break
                    
                    # ë°°ì¹˜ íŒŒì‹±
                    images, targets = self._parse_batch(batch)
                    if images is None:
                        continue
                    
                    # Student ì¶”ë¡ 
                    results = self.student.model(images)
                    
                    # ë””ë²„ê¹…: WandB ë¡œê¹…ì—ì„œë„ ëª¨ë¸ ì¶œë ¥ í™•ì¸
                    print(f"ğŸ” WandB ë¡œê¹… - ëª¨ë¸ ì¶œë ¥ íƒ€ì…: {type(results)}")
                    if hasattr(results, '__len__'):
                        print(f"ğŸ” WandB ë¡œê¹… - ì¶œë ¥ ê¸¸ì´: {len(results)}")
                    
                    # ê° ì´ë¯¸ì§€ ì²˜ë¦¬
                    for img_idx in range(min(images.shape[0], num_images - len(wandb_images))):
                        # ì›ë³¸ ì´ë¯¸ì§€ ë³µì› (0-1 ì •ê·œí™”ëœ ì´ë¯¸ì§€ë¥¼ 0-255ë¡œ)
                        img_tensor = images[img_idx].cpu()
                        img_np = (img_tensor.permute(1, 2, 0).numpy() * 255).astype(np.uint8)
                        
                        # matplotlib figure ìƒì„±
                        fig, ax = plt.subplots(1, 1, figsize=(10, 8))
                        ax.imshow(img_np)
                        ax.set_title(f'Epoch {epoch} - Image {batch_idx}_{img_idx}')
                        
                        # ì˜ˆì¸¡ ê²°ê³¼ bbox ê·¸ë¦¬ê¸° (ë¹¨ê°„ìƒ‰)
                        if hasattr(results, '__iter__') and not isinstance(results, torch.Tensor):
                            if img_idx < len(results):
                                result = results[img_idx]
                                if hasattr(result, 'boxes') and result.boxes is not None:
                                    boxes = result.boxes
                                    if len(boxes) > 0:
                                        pred_boxes = boxes.xyxy.cpu().numpy()
                                        pred_scores = boxes.conf.cpu().numpy()
                                        
                                        for j, (box, score) in enumerate(zip(pred_boxes, pred_scores)):
                                            if score > 0.3:  # ì‹ ë¢°ë„ ì„ê³„ê°’
                                                x1, y1, x2, y2 = box
                                                width = x2 - x1
                                                height = y2 - y1
                                                
                                                # ì˜ˆì¸¡ bbox (ë¹¨ê°„ìƒ‰)
                                                rect = patches.Rectangle(
                                                    (x1, y1), width, height,
                                                    linewidth=2, edgecolor='red', 
                                                    facecolor='none', linestyle='-'
                                                )
                                                ax.add_patch(rect)
                                                
                                                # ì‹ ë¢°ë„ ì ìˆ˜ í‘œì‹œ
                                                ax.text(x1, y1-5, f'Pred: {score:.2f}', 
                                                       color='red', fontsize=10, 
                                                       bbox=dict(boxstyle="round,pad=0.3", 
                                                               facecolor='white', alpha=0.7))
                        
                        # Ground Truth bbox ê·¸ë¦¬ê¸° (ì´ˆë¡ìƒ‰)
                        if isinstance(targets, dict) and 'bboxes' in targets:
                            batch_idx_tensor = targets.get('batch_idx', torch.arange(images.shape[0]))
                            bboxes = targets['bboxes']
                            
                            # í˜„ì¬ ì´ë¯¸ì§€ì˜ GT bbox ì°¾ê¸°
                            mask = batch_idx_tensor == img_idx
                            if mask.any():
                                img_bboxes = bboxes[mask].cpu().numpy()
                                
                                for k, bbox in enumerate(img_bboxes):
                                    # YOLO format (cx, cy, w, h) to (x1, y1, x2, y2)
                                    h, w = img_np.shape[:2]
                                    cx, cy, bw, bh = bbox
                                    x1 = (cx - bw/2) * w
                                    y1 = (cy - bh/2) * h
                                    x2 = (cx + bw/2) * w
                                    y2 = (cy + bh/2) * h
                                    
                                    width = x2 - x1
                                    height = y2 - y1
                                    
                                    # GT bbox (ì´ˆë¡ìƒ‰)
                                    rect = patches.Rectangle(
                                        (x1, y1), width, height,
                                        linewidth=2, edgecolor='green', 
                                        facecolor='none', linestyle='--'
                                    )
                                    ax.add_patch(rect)
                                    
                                    # GT ë¼ë²¨ í‘œì‹œ
                                    ax.text(x1, y2+15, f'GT', 
                                           color='green', fontsize=10,
                                           bbox=dict(boxstyle="round,pad=0.3", 
                                                   facecolor='white', alpha=0.7))
                        
                        # ë²”ë¡€ ì¶”ê°€
                        red_patch = patches.Patch(color='red', label='Prediction')
                        green_patch = patches.Patch(color='green', label='Ground Truth')
                        ax.legend(handles=[red_patch, green_patch], loc='upper right')
                        
                        ax.axis('off')
                        plt.tight_layout()
                        
                        # WandBì— ì¶”ê°€
                        wandb_images.append(wandb.Image(fig, caption=f"Epoch {epoch} - Inference {batch_idx}_{img_idx}"))
                        plt.close(fig)  # ë©”ëª¨ë¦¬ ì •ë¦¬
                        
                        if len(wandb_images) >= num_images:
                            break
            
            # WandBì— ë¡œê¹…
            if wandb_images:
                wandb.log({f"validation_images_epoch_{epoch}": wandb_images}, step=epoch)
                print(f"âœ… {len(wandb_images)}ê°œ ì´ë¯¸ì§€ë¥¼ WandBì— ë¡œê¹…í–ˆìŠµë‹ˆë‹¤.")
            else:
                print("âš ï¸ ë¡œê¹…í•  ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            self.student.model.train()
            
        except Exception as e:
            print(f"âŒ WandB ì´ë¯¸ì§€ ë¡œê¹… ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
