"""
Figma UI ê²€ì¦ì„ ìœ„í•œ YOLOv11 ì¦ë¥˜ ë©”ì¸ í´ë˜ìŠ¤
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from ultralytics import YOLO
import yaml
from pathlib import Path
from typing import Dict, Tuple, Optional, List
import numpy as np
import wandb
from datetime import datetime
import cv2
import os
from torch.utils.data import DataLoader
import multiprocessing as mp
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from PIL import Image
import random

from ..losses.single_class_loss import SingleClassDistillationLoss
from ..losses.feature_alignment_loss import FeatureAlignmentLoss


class FigmaUIDistillation:
    """Figma UI ê²€ì¦ì„ ìœ„í•œ YOLOv11 ì¦ë¥˜ ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self,
                 teacher_model: str = 'yolov11l.pt',
                 student_model: str = 'yolov11s.yaml',
                 data_yaml: str = 'figma_ui.yaml',
                 device: str = 'cuda',
                 use_wandb: bool = True,
                 verbose_debug: bool = False):
        """
        Args:
            teacher_model: Teacher ëª¨ë¸ ê²½ë¡œ (YOLOv11-l)
            student_model: Student ëª¨ë¸ ì„¤ì • (YOLOv11-s ë˜ëŠ” YOLOv11-m)
            data_yaml: Figma UI ë°ì´í„°ì…‹ ì„¤ì •
            device: í•™ìŠµ ë””ë°”ì´ìŠ¤
            use_wandb: WandB ì‚¬ìš© ì—¬ë¶€
            verbose_debug: ìƒì„¸ ë””ë²„ê¹… ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€
        """
        self.device = device
        self.use_wandb = use_wandb
        self.verbose_debug = verbose_debug
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        print(f"Loading teacher model: {teacher_model}")
        self.teacher = YOLO(teacher_model)
        self.teacher.model = self.teacher.model.to(self.device)
        self.teacher.model.eval()
        for param in self.teacher.model.parameters():
            param.requires_grad = False
        
        print(f"Initializing student model: {student_model}")
        self.student = YOLO(student_model)
        self.student.model = self.student.model.to(self.device)
        
        print(f"âœ… Models loaded on device: {self.device}")
        print(f"âœ… Teacher parameters: {sum(p.numel() for p in self.teacher.model.parameters()):,}")
        print(f"âœ… Student parameters: {sum(p.numel() for p in self.student.model.parameters()):,}")
        
        # ë‹¨ì¼ í´ë˜ìŠ¤ ì„¤ì •
        self.num_classes = 1  # Figma UI component
        
        # ì†ì‹¤ í•¨ìˆ˜
        self.distillation_loss = SingleClassDistillationLoss(
            bbox_weight=2.0,
            objectness_weight=1.0,
            feature_weight=1.0
        )
        self.feature_loss = FeatureAlignmentLoss(
            use_attention=True
        )
        
        # ë°ì´í„° ì„¤ì •
        self.data_yaml = data_yaml
        self._modify_data_for_single_class()
        
        # WandB ì´ˆê¸°í™”
        if self.use_wandb:
            self.init_wandb()
    
    def _modify_data_for_single_class(self):
        """ë°ì´í„°ì…‹ì„ ë‹¨ì¼ í´ë˜ìŠ¤ë¡œ ìˆ˜ì •"""
        with open(self.data_yaml, 'r') as f:
            data = yaml.safe_load(f)
        
        # ë‹¨ì¼ í´ë˜ìŠ¤ë¡œ ì„¤ì •
        data['nc'] = 1
        data['names'] = ['ui_component']
        
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        self.modified_data_yaml = 'temp_single_class_data.yaml'
        with open(self.modified_data_yaml, 'w') as f:
            yaml.dump(data, f)
    
    def init_wandb(self):
        """WandB ì´ˆê¸°í™” ë° ì„¤ì •"""
        config = {
            'project': 'figma-ui-detection',
            'teacher_model': 'YOLOv11-l',
            'student_model': self.student.model.__class__.__name__,
            'num_classes': self.num_classes,
            'task': 'single_class_detection',
            'bbox_weight': self.distillation_loss.bbox_weight,
            'objectness_weight': self.distillation_loss.objectness_weight,
            'feature_weight': self.distillation_loss.feature_weight
        }
        
        wandb.init(
            project="figma-ui-yolo11-kd",
            name=f"distill_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            config=config,
            tags=["single-class", "figma-ui", "knowledge-distillation"]
        )
        
        # ëª¨ë¸ ë³µì¡ë„ ë¹„êµ ë¡œê¹…
        self.log_model_comparison()
    
    def log_model_comparison(self):
        """Teacher vs Student ëª¨ë¸ ë¹„êµ ë¡œê¹…"""
        if not self.use_wandb:
            return
        
        teacher_params = sum(p.numel() for p in self.teacher.model.parameters())
        student_params = sum(p.numel() for p in self.student.model.parameters())
        
        comparison_table = wandb.Table(
            columns=["Model", "Parameters", "Size (MB)", "Compression Ratio"],
            data=[
                ["Teacher (YOLOv11-l)", teacher_params, teacher_params * 4 / 1e6, 1.0],
                ["Student", student_params, student_params * 4 / 1e6, 
                 teacher_params / student_params]
            ]
        )
        
        wandb.log({
            "model_comparison": comparison_table,
            "compression_ratio": teacher_params / student_params
        })
    
    def extract_features(self, model, x):
        """ëª¨ë¸ì—ì„œ ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ íŠ¹ì§• ì¶”ì¶œ"""
        features = []
        
        # YOLOv11 ë°±ë³¸ì—ì„œ íŠ¹ì§• ì¶”ì¶œ
        # P3, P4, P5 ë ˆë²¨ íŠ¹ì§•
        for i, module in enumerate(model.model):
            x = module(x)
            if i in [4, 6, 9]:  # ì£¼ìš” íŠ¹ì§• ë ˆì´ì–´
                features.append(x)
        
        return features, x
    
    def train_step(self, batch, optimizer):
        """ë‹¨ì¼ í•™ìŠµ ìŠ¤í…"""
        # ë°°ì¹˜ êµ¬ì¡° í™•ì¸ ë° ì•ˆì „í•œ ì²˜ë¦¬
        if isinstance(batch, dict):
            images = batch['img']
            # ì´ë¯¸ì§€ ë°ì´í„° íƒ€ì… ë° ì •ê·œí™” ì²˜ë¦¬
            if images.dtype == torch.uint8:
                images = images.float() / 255.0  # uint8 -> float32, [0,255] -> [0,1]
            images = images.to(self.device)
            
            # print(f"ğŸ” ì´ë¯¸ì§€ shape: {images.shape}, dtype: {images.dtype}, device: {images.device}")
            
            # YOLO ë°°ì¹˜ì—ì„œ íƒ€ê²Ÿ ì¶”ì¶œ (ë‹¤ì–‘í•œ í‚¤ í™•ì¸)
            if 'batch_idx' in batch:
                targets = batch  # ì „ì²´ ë°°ì¹˜ ì •ë³´
            elif 'cls' in batch and 'bboxes' in batch:
                targets = batch  # ë¶„ë¦¬ëœ í˜•íƒœ
            else:
                # ë°°ì¹˜ í‚¤ í™•ì¸ì„ ìœ„í•œ ë””ë²„ê¹…
                # print(f"ğŸ” ë°°ì¹˜ í‚¤: {list(batch.keys())}")
                targets = batch
        else:
            # ë°°ì¹˜ê°€ íŠœí”Œì´ë‚˜ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°
            images = batch[0]
            if images.dtype == torch.uint8:
                images = images.float() / 255.0
            images = images.to(self.device)
            targets = batch[1] if len(batch) > 1 else None
        
        # ëª¨ë¸ ë””ë°”ì´ìŠ¤ í™•ì¸
        # print(f"ğŸ” Teacher model device: {next(self.teacher.model.parameters()).device}")
        # print(f"ğŸ” Student model device: {next(self.student.model.parameters()).device}")
        
        # ëª¨ë¸ì„ ì˜¬ë°”ë¥¸ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        self.teacher.model = self.teacher.model.to(self.device)
        self.student.model = self.student.model.to(self.device)
        
        try:
            # Teacher ì¶”ë¡  (no gradient)
            with torch.no_grad():
                # print("ğŸ” Teacher ì¶”ë¡  ì‹œì‘...")
                teacher_outputs = self.teacher.model(images)
                # print(f"ğŸ” Teacher ì¶œë ¥ íƒ€ì…: {type(teacher_outputs)}")
                if isinstance(teacher_outputs, (list, tuple)):
                    # print(f"ğŸ” Teacher ì¶œë ¥ ê°œìˆ˜: {len(teacher_outputs)}")
                    teacher_outputs = teacher_outputs[0] if len(teacher_outputs) > 0 else teacher_outputs
                teacher_outputs = self.parse_model_outputs(teacher_outputs)
                teacher_features = []  # ì„ì‹œë¡œ ë¹ˆ ë¦¬ìŠ¤íŠ¸
            
            # Student ì¶”ë¡ 
            # print("ğŸ” Student ì¶”ë¡  ì‹œì‘...")
            raw_student_preds = self.student.model(images)  # ì›ë³¸ ì˜ˆì¸¡ ë³´ê´€
            # print(f"ğŸ” Student ì¶œë ¥ íƒ€ì…: {type(raw_student_preds)}")
            
            # ì›ë³¸ ì˜ˆì¸¡ êµ¬ì¡° ìƒì„¸ ë””ë²„ê¹…
            if isinstance(raw_student_preds, (list, tuple)):
                # print(f"ğŸ” Student ì¶œë ¥ ê°œìˆ˜: {len(raw_student_preds)}")
                for i, item in enumerate(raw_student_preds):
                    # print(f"ğŸ” Student ì¶œë ¥[{i}] íƒ€ì…: {type(item)}")
                    if hasattr(item, 'shape'):
                        # print(f"ğŸ” Student ì¶œë ¥[{i}] í˜•íƒœ: {item.shape}")
                
                # í…ì„œë§Œ í•„í„°ë§í•˜ì—¬ YOLO ì†ì‹¤ì— ì „ë‹¬
                tensor_preds = [item for item in raw_student_preds if hasattr(item, 'view')]
                # print(f"ğŸ” í…ì„œ ì˜ˆì¸¡ ê°œìˆ˜: {len(tensor_preds)}")
                
                student_outputs_for_parsing = raw_student_preds[0] if len(raw_student_preds) > 0 else raw_student_preds
            else:
                # print(f"ğŸ” Student ë‹¨ì¼ ì¶œë ¥ í˜•íƒœ: {getattr(raw_student_preds, 'shape', 'No shape')}")
                tensor_preds = raw_student_preds
                student_outputs_for_parsing = raw_student_preds
                
            student_outputs = self.parse_model_outputs(student_outputs_for_parsing)  # KDìš© íŒŒì‹±
            student_features = []  # ì„ì‹œë¡œ ë¹ˆ ë¦¬ìŠ¤íŠ¸
            
        except Exception as model_error:
            print(f"âŒ ëª¨ë¸ ì¶”ë¡  ì¤‘ ì˜¤ë¥˜: {model_error}")
            print(f"âŒ ì˜¤ë¥˜ íƒ€ì…: {type(model_error)}")
            raise model_error
        
        # 1. Detection ì¦ë¥˜ ì†ì‹¤ (objectness + bbox)
        try:
            det_loss, det_metrics = self.distillation_loss(
                student_outputs, teacher_outputs, targets
            )
            print(f"âœ… Detection ì†ì‹¤ ê³„ì‚° ì„±ê³µ: {det_loss.item():.4f}")
        except Exception as det_error:
            import traceback
            print(f"âŒ Detection ì†ì‹¤ ê³„ì‚° ì˜¤ë¥˜:")
            print(f"   ì˜¤ë¥˜ íƒ€ì…: {type(det_error).__name__}")
            print(f"   ì˜¤ë¥˜ ë©”ì‹œì§€: {str(det_error)}")
            print(f"   ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:")
            print(traceback.format_exc())
            raise det_error
        
        # 2. Feature ì¦ë¥˜ ì†ì‹¤
        try:
            feat_loss, feat_metrics = self.feature_loss(
                student_features, teacher_features
            )
            print(f"âœ… Feature ì†ì‹¤ ê³„ì‚° ì„±ê³µ: {feat_loss.item():.4f}")
        except Exception as feat_error:
            import traceback
            print(f"âŒ Feature ì†ì‹¤ ê³„ì‚° ì˜¤ë¥˜:")
            print(f"   ì˜¤ë¥˜ íƒ€ì…: {type(feat_error).__name__}")  
            print(f"   ì˜¤ë¥˜ ë©”ì‹œì§€: {str(feat_error)}")
            print(f"   ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:")
            print(traceback.format_exc())
            raise feat_error
        
        # 3. ì›ë³¸ YOLO ì†ì‹¤ (Ground Truth ê¸°ë°˜) - ì„ì‹œ ë¹„í™œì„±í™”
        try:
            # YOLO ì†ì‹¤ í•¨ìˆ˜ì™€ì˜ í˜¸í™˜ì„± ë¬¸ì œë¡œ ì¸í•´ ì„ì‹œë¡œ ë¹„í™œì„±í™”
            # Knowledge Distillation ì†ì‹¤ë§Œ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ì§„í–‰
            print("âš ï¸ Base ì†ì‹¤ ê³„ì‚°ì„ ì„ì‹œë¡œ ê±´ë„ˆë›°ê³  KD ì†ì‹¤ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤")
            base_loss = torch.tensor(0.0, device=images.device)
            print(f"âœ… Base ì†ì‹¤ (ë¹„í™œì„±í™”): {base_loss.item():.4f}")
        except Exception as base_error:
            import traceback
            print(f"âŒ Base ì†ì‹¤ ê³„ì‚° ì˜¤ë¥˜:")
            print(f"   ì˜¤ë¥˜ íƒ€ì…: {type(base_error).__name__}")
            print(f"   ì˜¤ë¥˜ ë©”ì‹œì§€: {str(base_error)}")
            print(f"   ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:")
            print(traceback.format_exc())
            # Base ì†ì‹¤ ì‹¤íŒ¨ ì‹œì—ë„ í•™ìŠµ ê³„ì† ì§„í–‰
            base_loss = torch.tensor(0.0, device=images.device)
            print("âš ï¸ Base ì†ì‹¤ ê³„ì‚° ì‹¤íŒ¨ - 0ìœ¼ë¡œ ì„¤ì •í•˜ê³  í•™ìŠµ ê³„ì†")
        
        # ì „ì²´ ì†ì‹¤ ì¡°í•© (Base ì†ì‹¤ ë¹„í™œì„±í™”ë¡œ ì¸í•´ ê°€ì¤‘ì¹˜ ì¡°ì •)
        total_loss = det_loss + 0.5 * feat_loss + 0.0 * base_loss  # Base ì†ì‹¤ ê°€ì¤‘ì¹˜ 0ìœ¼ë¡œ ì„¤ì •
        
        # ì—­ì „íŒŒ
        optimizer.zero_grad()
        total_loss.backward()
        optimizer.step()
        
        # ë©”íŠ¸ë¦­ ë¡œê¹…
        metrics = {
            'loss/total': total_loss.item(),
            'loss/detection': det_loss.item(),
            'loss/feature': feat_loss.item(),
            'loss/base': base_loss.item(),
            **det_metrics,
            **feat_metrics
        }
        
        return metrics
    
    def debug_print(self, message: str, level: str = "info"):
        """ì¡°ê±´ë¶€ ë””ë²„ê¹… ì¶œë ¥"""
        if self.verbose_debug:
            prefix = "ğŸ”" if level == "info" else "âš ï¸" if level == "warning" else "âŒ"
            print(f"{prefix} {message}")
    
    def parse_model_outputs(self, outputs):
        """ëª¨ë¸ ì¶œë ¥ì„ íŒŒì‹±í•˜ì—¬ bboxì™€ objectness ë¶„ë¦¬"""
        self.debug_print(f"ì¶œë ¥ íŒŒì‹± - shape: {outputs.shape if hasattr(outputs, 'shape') else 'No shape'}")
        self.debug_print(f"ì¶œë ¥ íŒŒì‹± - type: {type(outputs)}")
        
        # YOLOv11 ì¶œë ¥ í˜•ì‹ ì²˜ë¦¬
        if outputs.dim() == 3:  # [batch, num_anchors, features]
            if outputs.shape[-1] >= 5:  # [x, y, w, h, conf, ...]
                return {
                    'bbox': outputs[..., :4],
                    'objectness': outputs[..., 4:5]
                }
            else:
                self.debug_print(f"ì˜ˆìƒê³¼ ë‹¤ë¥¸ ì¶œë ¥ ì°¨ì›: {outputs.shape}", "warning")
                # ê¸°ë³¸ê°’ ë°˜í™˜
                batch_size = outputs.shape[0]
                return {
                    'bbox': torch.zeros(batch_size, 1, 4, device=outputs.device),
                    'objectness': torch.zeros(batch_size, 1, 1, device=outputs.device)
                }
        elif outputs.dim() == 4:  # [batch, features, height, width]
            self.debug_print(f"4D ì¶œë ¥ ê°ì§€: {outputs.shape}")
            # Feature map í˜•íƒœì¸ ê²½ìš° reshape í•„ìš”
            batch_size, features, h, w = outputs.shape
            # Flatten spatial dimensions
            outputs_flat = outputs.view(batch_size, features, h * w).transpose(1, 2)  # [B, H*W, features]
            
            if features >= 5:
                return {
                    'bbox': outputs_flat[..., :4],
                    'objectness': outputs_flat[..., 4:5]
                }
            else:
                # ê¸°ë³¸ê°’ ë°˜í™˜
                return {
                    'bbox': torch.zeros(batch_size, h*w, 4, device=outputs.device),
                    'objectness': torch.zeros(batch_size, h*w, 1, device=outputs.device)
                }
        else:
            self.debug_print(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¶œë ¥ ì°¨ì›: {outputs.shape}", "error")
            batch_size = outputs.shape[0] if outputs.dim() > 0 else 1
            return {
                'bbox': torch.zeros(batch_size, 1, 4, device=outputs.device),
                'objectness': torch.zeros(batch_size, 1, 1, device=outputs.device)
            }
    
    def validate(self, val_loader):
        """ê²€ì¦ ìˆ˜í–‰ - ì‹¤ì œ mAP ê³„ì‚°"""
        if val_loader is None:
            return {
                'val/mAP': 0.0,
                'val/precision': 0.0,
                'val/recall': 0.0,
                'val/inference_time': 0.0
            }
        
        self.student.model.eval()
        
        try:
            # ë¹ ë¥¸ í‰ê°€ (ì²˜ìŒ 20ë°°ì¹˜ë§Œ ì‚¬ìš©)
            eval_metrics = self.evaluate_model(val_loader, epoch=0)
            
            # ì¶”ë¡  ì‹œê°„ ì¸¡ì •
            total_time = 0
            num_batches = 0
            
            with torch.no_grad():
                for batch_idx, batch in enumerate(val_loader):
                    if batch_idx >= 10:  # ì‹œê°„ ì¸¡ì •ìš© 10ë°°ì¹˜ë§Œ
                        break
                        
                    if isinstance(batch, dict):
                        images = batch.get('img', batch.get('image'))
                    elif isinstance(batch, (tuple, list)) and len(batch) >= 2:
                        images = batch[0]
                    else:
                        continue
                    
                    if images is None:
                        continue
                        
                    images = images.float() / 255.0 if images.dtype == torch.uint8 else images
                    images = images.to(self.device)
                    
                    start_time = time.time()
                    _ = self.student.model(images)
                    total_time += time.time() - start_time
                    num_batches += 1
            
            avg_time = total_time / num_batches if num_batches > 0 else 0
            
            self.student.model.train()
            return {
                'val/mAP': eval_metrics.get('map50', 0.0),
                'val/mAP_50_95': eval_metrics.get('map', 0.0),
                'val/precision': eval_metrics.get('precision', 0.0),
                'val/recall': eval_metrics.get('recall', 0.0),
                'val/inference_time': avg_time
            }
            
        except Exception as e:
            print(f"âš ï¸ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            self.student.model.train()
            return {
                'val/mAP': 0.0,
                'val/precision': 0.0,
                'val/recall': 0.0,
                'val/inference_time': 0.0
            }
    
    def train(self,
              epochs: int = 100,
              batch_size: int = 16,
              learning_rate: float = 0.001,
              num_workers: int = 8,
              save_dir: str = './runs/figma_distillation'):
        """
        ì „ì²´ í•™ìŠµ ì‹¤í–‰
        
        Args:
            epochs: í•™ìŠµ ì—í­
            batch_size: ë°°ì¹˜ í¬ê¸°
            learning_rate: í•™ìŠµë¥ 
            num_workers: ë°ì´í„°ë¡œë” ì›Œì»¤ ìˆ˜
            save_dir: ëª¨ë¸ ì €ì¥ ê²½ë¡œ
        """
        
        # ì˜µí‹°ë§ˆì´ì €
        optimizer = torch.optim.AdamW(
            self.student.model.parameters(),
            lr=learning_rate,
            weight_decay=0.0005
        )
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ (Cosine Annealing)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=epochs
        )
        
        # ë°ì´í„°ë¡œë” ìƒì„± ì‹œë„
        train_loader, val_loader = self._create_dataloaders(batch_size, num_workers)
        
        # ë°ì´í„°ë¡œë”ê°€ ì—†ëŠ” ê²½ìš° YOLO ê¸°ë³¸ í•™ìŠµ ë°©ì‹ ì‚¬ìš©
        if train_loader is None:
            print("ì»¤ìŠ¤í…€ ë°ì´í„°ë¡œë” ìƒì„± ì‹¤íŒ¨. YOLO ê¸°ë³¸ í•™ìŠµ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return 0
            # return self._train_with_yolo_default(epochs, batch_size, learning_rate, save_dir)
        
        best_map = 0
        
        for epoch in range(epochs):
            print(f"\n[Epoch {epoch+1}/{epochs}]")
            
            # í•™ìŠµ
            epoch_metrics = {}
            for batch_idx, batch in enumerate(train_loader):
                try:
                    metrics = self.train_step(batch, optimizer)
                    
                    # ë°°ì¹˜ ë©”íŠ¸ë¦­ ëˆ„ì 
                    for k, v in metrics.items():
                        if k not in epoch_metrics:
                            epoch_metrics[k] = 0
                        epoch_metrics[k] += v
                    
                    # ì£¼ê¸°ì  ë¡œê¹… (ë” ê°„ê²°í•˜ê²Œ)
                    if batch_idx % 20 == 0:
                        print(f"  ğŸ“Š Batch {batch_idx}: Loss = {metrics.get('loss/total', 0):.4f}")
                        if self.use_wandb:
                            wandb.log(metrics, step=epoch * len(train_loader) + batch_idx)
                            
                except Exception as batch_error:
                    import traceback
                    print(f"âŒ Batch {batch_idx} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜:")
                    print(f"   ì˜¤ë¥˜ íƒ€ì…: {type(batch_error).__name__}")
                    print(f"   ì˜¤ë¥˜ ë©”ì‹œì§€: {str(batch_error)}")
                    print(f"   ìƒì„¸ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:")
                    print(traceback.format_exc())
                    
                    if batch_idx == 0:  # ì²« ë²ˆì§¸ ë°°ì¹˜ì—ì„œ ì˜¤ë¥˜ë©´ ì¤‘ë‹¨
                        print("ì²« ë²ˆì§¸ ë°°ì¹˜ë¶€í„° ì˜¤ë¥˜ ë°œìƒ. í•™ìŠµì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                        return 0
                    continue  # ë‹¤ë¥¸ ë°°ì¹˜ëŠ” ê±´ë„ˆë›°ê¸°
            
            # ê²€ì¦ ë° í‰ê°€
            val_metrics = self.validate(val_loader)
            
            # mAP í‰ê°€ (ë§¤ 5 ì—í­ë§ˆë‹¤)
            if val_loader is not None and (epoch + 1) % 5 == 0:
                try:
                    eval_metrics = self.evaluate_model(val_loader, epoch + 1)
                    val_metrics.update({f'eval_{k}': v for k, v in eval_metrics.items()})
                    
                    # ì¶”ë¡  ì´ë¯¸ì§€ ë¡œê¹… (ë§¤ 10 ì—í­ë§ˆë‹¤)
                    if (epoch + 1) % 10 == 0:
                        self.log_inference_images(val_loader, epoch + 1, num_images=3)
                        
                except Exception as eval_error:
                    print(f"âš ï¸ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {eval_error}")
            
            # ì—í­ í‰ê·  ë©”íŠ¸ë¦­
            for k in epoch_metrics:
                epoch_metrics[k] /= len(train_loader)
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
            scheduler.step()
            
            # WandB ë¡œê¹…
            if self.use_wandb:
                wandb.log({
                    'epoch': epoch + 1,
                    **epoch_metrics,
                    **val_metrics,
                    'lr': scheduler.get_last_lr()[0]
                })
            
            # ì—í­ ê²°ê³¼ í‘œì‹œ
            print(f"ğŸ“ˆ Epoch {epoch+1}/{epochs} ì™„ë£Œ:")
            print(f"   ğŸ”¥ í‰ê·  ì†ì‹¤: {epoch_metrics['loss/total']:.4f}")
            print(f"   ğŸ¯ mAP@50: {val_metrics.get('val/mAP', 0):.4f}")
            print(f"   âš¡ ì¶”ë¡  ì‹œê°„: {val_metrics.get('val/inference_time', 0)*1000:.1f}ms")
            
            # ëª¨ë¸ ì €ì¥
            if val_metrics['val/mAP'] > best_map:
                best_map = val_metrics['val/mAP']
                save_path = f"{save_dir}/best_model.pt"
                torch.save(self.student.model.state_dict(), save_path)
                print(f"ğŸŒŸ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! mAP: {best_map:.4f}")
                
                if self.use_wandb:
                    wandb.save(save_path)
            
            print("-" * 40)
        
        return best_map
    
    def _train_with_yolo_default(self, epochs, batch_size, learning_rate, save_dir):
        """
        YOLO ê¸°ë³¸ í•™ìŠµ ë°©ì‹ì„ ì‚¬ìš©í•œ Knowledge Distillation
        """
        print("YOLO ê¸°ë³¸ í•™ìŠµ ë°©ì‹ìœ¼ë¡œ Knowledge Distillationì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        # YOLO ê¸°ë³¸ í•™ìŠµ ì„¤ì •
        results = self.student.train(
            data=self.modified_data_yaml,
            epochs=epochs,
            batch=batch_size,
            lr0=learning_rate,
            device=self.device,
            project=save_dir,
            name='distillation_run',
            save=True,
            plots=True,
            verbose=True
        )
        
        # ê²°ê³¼ì—ì„œ ìµœê³  mAP ì¶”ì¶œ
        if hasattr(results, 'results_dict'):
            best_map = results.results_dict.get('metrics/mAP50-95(B)', 0.0)
        else:
            best_map = 0.0
            
        print(f"YOLO ê¸°ë³¸ í•™ìŠµ ì™„ë£Œ! Best mAP: {best_map:.4f}")
        return best_map
    
    def _create_dataloaders(self, batch_size, num_workers):
        """
        YOLO ë°ì´í„°ë¡œë”ë¥¼ ì§ì ‘ ìƒì„±
        """
        try:
            from ultralytics.data.dataset import YOLODataset
            from torch.utils.data import DataLoader
            import yaml
            import os
            from pathlib import Path
            
            print("ğŸ”„ ë²„ì „ í˜¸í™˜ì„± ë¬¸ì œë¥¼ í”¼í•œ ì§ì ‘ ë°ì´í„°ë¡œë” ìƒì„±ì„ ì‹œë„í•©ë‹ˆë‹¤...")
            
            # ë°ì´í„°ì…‹ ì„¤ì • ë¡œë“œ
            with open(self.modified_data_yaml, 'r') as f:
                data_config = yaml.safe_load(f)
            
            # ë°ì´í„°ì…‹ ê²½ë¡œ í™•ì¸
            data_path = data_config.get('path', './datasets/figma_ui')
            train_path = os.path.join(data_path, data_config.get('train', 'images/train'))
            val_path = os.path.join(data_path, data_config.get('val', 'images/val'))
            
            print(f"ğŸ“ ë°ì´í„°ì…‹ ê²½ë¡œ: {data_path}")
            print(f"ğŸ“ í•™ìŠµ ê²½ë¡œ: {train_path}")
            print(f"ğŸ“ ê²€ì¦ ê²½ë¡œ: {val_path}")
            
            # ê²½ë¡œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            if not os.path.exists(train_path):
                print(f"âŒ í•™ìŠµ ë°ì´í„° ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {train_path}")
                return None, None
            
            # ì´ë¯¸ì§€ íŒŒì¼ í™•ì¸
            image_files = []
            for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp']:
                image_files.extend(Path(train_path).glob(ext))
            
            if not image_files:
                print(f"âŒ {train_path}ì—ì„œ ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None, None
            
            print(f"âœ… {len(image_files)}ê°œì˜ ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            
            # ì§ì ‘ YOLO ë°ì´í„°ì…‹ ìƒì„± (ë²„ì „ í˜¸í™˜ì„± ë¬¸ì œ íšŒí”¼)
            try:
                # í•™ìŠµìš© ë°ì´í„°ì…‹
                train_dataset = YOLODataset(
                    img_path=train_path,
                    data=data_config,
                    task='detect',
                    imgsz=640,
                    batch_size=batch_size,
                    augment=True,
                    cache=False,
                    single_cls=True,  # ë‹¨ì¼ í´ë˜ìŠ¤
                    stride=32,
                    pad=0.0,
                    rect=False
                )
                
                train_loader = DataLoader(
                    train_dataset,
                    batch_size=batch_size,
                    shuffle=True,
                    num_workers=num_workers,
                    pin_memory=True,
                    drop_last=True,
                    collate_fn=train_dataset.collate_fn
                )
                
                print(f"âœ… í•™ìŠµ ë°ì´í„°ë¡œë” ìƒì„± ì„±ê³µ: {len(train_loader)} batches")
                
                # ê²€ì¦ìš© ë°ì´í„°ì…‹
                val_loader = None
                if os.path.exists(val_path):
                    try:
                        val_dataset = YOLODataset(
                            img_path=val_path,
                            data=data_config,
                            task='detect',
                            imgsz=640,
                            batch_size=batch_size,
                            augment=False,
                            cache=False,
                            single_cls=True,
                            stride=32,
                            pad=0.5,
                            rect=True
                        )
                        
                        val_loader = DataLoader(
                            val_dataset,
                            batch_size=batch_size,
                            shuffle=False,
                            num_workers=num_workers,
                            pin_memory=True,
                            drop_last=False,
                            collate_fn=val_dataset.collate_fn
                        )
                        
                        print(f"âœ… ê²€ì¦ ë°ì´í„°ë¡œë” ìƒì„± ì„±ê³µ: {len(val_loader)} batches")
                    except Exception as val_e:
                        print(f"âš ï¸ ê²€ì¦ ë°ì´í„°ë¡œë” ìƒì„± ì‹¤íŒ¨: {val_e}")
                
                print("ğŸ‰ ì§ì ‘ ë°ì´í„°ë¡œë” ìƒì„± ì™„ë£Œ!")
                return train_loader, val_loader
                
            except Exception as dataset_e:
                print(f"âŒ YOLO ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨: {dataset_e}")
                return None, None
                
        except ImportError as ie:
            print(f"âŒ YOLO ëª¨ë“ˆ import ì˜¤ë¥˜: {ie}")
            return None, None
        except Exception as e:
            print(f"âŒ ë°ì´í„°ë¡œë” ìƒì„± ì¤‘ ì „ì²´ ì˜¤ë¥˜: {e}")
            print("ğŸ’¡ ì§ì ‘ ë°ì´í„°ë¡œë” ìƒì„±ë„ ì‹¤íŒ¨ - YOLO ê¸°ë³¸ í•™ìŠµìœ¼ë¡œ í´ë°±í•©ë‹ˆë‹¤")
            return None, None
    
    def evaluate_model(self, val_loader: DataLoader, epoch: int) -> Dict:
        """ëª¨ë¸ í‰ê°€ ë° mAP ê³„ì‚°"""
        self.student.model.eval()
        
        all_predictions = []
        all_targets = []
        
        print(f"\nğŸ“Š Epoch {epoch} ëª¨ë¸ í‰ê°€ ì‹œì‘...")
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(val_loader):
                if batch_idx >= 50:  # ë¹ ë¥¸ í‰ê°€ë¥¼ ìœ„í•´ 50ë°°ì¹˜ë§Œ ì‚¬ìš©
                    break
                    
                # ë°°ì¹˜ íŒŒì‹±
                if isinstance(batch, dict):
                    images = batch.get('img', batch.get('image'))
                    targets = batch
                elif isinstance(batch, (tuple, list)) and len(batch) >= 2:
                    images, targets = batch[0], batch[1]
                else:
                    continue
                
                if images is None:
                    continue
                
                images = images.float() / 255.0 if images.dtype == torch.uint8 else images
                images = images.to(self.device)
                
                # Student ì¶”ë¡ 
                try:
                    results = self.student.model(images)
                    # YOLO ê²°ê³¼ë¥¼ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    for i, result in enumerate(results):
                        if hasattr(result, 'boxes') and result.boxes is not None:
                            boxes = result.boxes
                            if len(boxes) > 0:
                                pred_boxes = boxes.xyxy.cpu().numpy()
                                pred_scores = boxes.conf.cpu().numpy()
                                pred_labels = boxes.cls.cpu().numpy()
                                
                                for j in range(len(pred_boxes)):
                                    all_predictions.append({
                                        'image_id': batch_idx * images.shape[0] + i,
                                        'bbox': pred_boxes[j],
                                        'score': pred_scores[j],
                                        'label': pred_labels[j]
                                    })
                    
                    # Ground Truth ì²˜ë¦¬
                    if isinstance(targets, dict) and 'bboxes' in targets:
                        batch_idx_tensor = targets.get('batch_idx', torch.arange(images.shape[0]))
                        bboxes = targets['bboxes']
                        cls = targets.get('cls', torch.zeros(len(bboxes)))
                        
                        for img_idx in range(images.shape[0]):
                            mask = batch_idx_tensor == img_idx
                            if mask.any():
                                img_bboxes = bboxes[mask].cpu().numpy()
                                img_cls = cls[mask].cpu().numpy()
                                
                                for k in range(len(img_bboxes)):
                                    # YOLO format (cx, cy, w, h) to (x1, y1, x2, y2)
                                    bbox = img_bboxes[k]
                                    h, w = images.shape[2], images.shape[3]
                                    x1 = (bbox[0] - bbox[2]/2) * w
                                    y1 = (bbox[1] - bbox[3]/2) * h
                                    x2 = (bbox[0] + bbox[2]/2) * w
                                    y2 = (bbox[1] + bbox[3]/2) * h
                                    
                                    all_targets.append({
                                        'image_id': batch_idx * images.shape[0] + img_idx,
                                        'bbox': [x1, y1, x2, y2],
                                        'label': img_cls[k]
                                    })
                
                except Exception as e:
                    print(f"âš ï¸ í‰ê°€ ì¤‘ ì˜¤ë¥˜ (ë°°ì¹˜ {batch_idx}): {e}")
                    continue
        
        # mAP ê³„ì‚°
        metrics = self.calculate_map(all_predictions, all_targets)
        
        print(f"ğŸ“Š í‰ê°€ ì™„ë£Œ:")
        print(f"   - ì´ ì˜ˆì¸¡: {len(all_predictions)}ê°œ")
        print(f"   - ì´ GT: {len(all_targets)}ê°œ") 
        print(f"   - mAP@0.5: {metrics.get('map50', 0.0):.4f}")
        print(f"   - mAP@0.5:0.95: {metrics.get('map', 0.0):.4f}")
        
        return metrics
    
    def calculate_map(self, predictions: List[Dict], targets: List[Dict]) -> Dict:
        """mAP ê³„ì‚°"""
        if not predictions or not targets:
            return {'map': 0.0, 'map50': 0.0, 'precision': 0.0, 'recall': 0.0}
        
        # IoU ì„ê³„ê°’ë“¤
        iou_thresholds = np.arange(0.5, 1.0, 0.05)
        
        # ì´ë¯¸ì§€ë³„ë¡œ ê·¸ë£¹í™”
        pred_by_image = {}
        gt_by_image = {}
        
        for pred in predictions:
            img_id = pred['image_id']
            if img_id not in pred_by_image:
                pred_by_image[img_id] = []
            pred_by_image[img_id].append(pred)
        
        for gt in targets:
            img_id = gt['image_id']
            if img_id not in gt_by_image:
                gt_by_image[img_id] = []
            gt_by_image[img_id].append(gt)
        
        # ê° IoU ì„ê³„ê°’ì—ì„œ AP ê³„ì‚°
        aps = []
        for iou_thresh in iou_thresholds:
            ap = self.calculate_ap_at_iou(pred_by_image, gt_by_image, iou_thresh)
            aps.append(ap)
        
        # mAP@0.5
        map50 = self.calculate_ap_at_iou(pred_by_image, gt_by_image, 0.5)
        
        # mAP@0.5:0.95
        map_avg = np.mean(aps)
        
        # Precision, Recall at IoU=0.5
        precision, recall = self.calculate_precision_recall(pred_by_image, gt_by_image, 0.5)
        
        return {
            'map': map_avg,
            'map50': map50,
            'precision': precision,
            'recall': recall
        }
    
    def calculate_ap_at_iou(self, pred_by_image: Dict, gt_by_image: Dict, iou_thresh: float) -> float:
        """íŠ¹ì • IoU ì„ê³„ê°’ì—ì„œ AP ê³„ì‚°"""
        all_scores = []
        all_matches = []
        total_gt = 0
        
        for img_id in gt_by_image.keys():
            gt_boxes = gt_by_image[img_id]
            pred_boxes = pred_by_image.get(img_id, [])
            
            total_gt += len(gt_boxes)
            
            if not pred_boxes:
                continue
            
            # Scoreë¡œ ì •ë ¬
            pred_boxes = sorted(pred_boxes, key=lambda x: x['score'], reverse=True)
            
            # GT ë§¤ì¹­ ì—¬ë¶€
            gt_matched = [False] * len(gt_boxes)
            
            for pred in pred_boxes:
                all_scores.append(pred['score'])
                
                # ìµœê³  IoU GT ì°¾ê¸°
                best_iou = 0
                best_gt_idx = -1
                
                for gt_idx, gt in enumerate(gt_boxes):
                    if gt_matched[gt_idx]:
                        continue
                    
                    iou = self.calculate_bbox_iou(pred['bbox'], gt['bbox'])
                    if iou > best_iou:
                        best_iou = iou
                        best_gt_idx = gt_idx
                
                # ë§¤ì¹­ ì—¬ë¶€ ê²°ì •
                if best_iou >= iou_thresh and best_gt_idx >= 0:
                    gt_matched[best_gt_idx] = True
                    all_matches.append(True)
                else:
                    all_matches.append(False)
        
        if not all_scores:
            return 0.0
        
        # Precision-Recall ì»¤ë¸Œ ê³„ì‚°
        sorted_indices = np.argsort(all_scores)[::-1]
        matches = np.array(all_matches)[sorted_indices]
        
        tp = np.cumsum(matches)
        fp = np.cumsum(~matches)
        
        precision = tp / (tp + fp)
        recall = tp / total_gt if total_gt > 0 else np.zeros_like(tp)
        
        # AP ê³„ì‚° (11-point interpolation)
        ap = 0
        for r in np.arange(0, 1.1, 0.1):
            p_max = np.max(precision[recall >= r]) if np.any(recall >= r) else 0
            ap += p_max / 11
        
        return ap
    
    def calculate_precision_recall(self, pred_by_image: Dict, gt_by_image: Dict, iou_thresh: float) -> Tuple[float, float]:
        """Precisionê³¼ Recall ê³„ì‚°"""
        total_tp = 0
        total_fp = 0
        total_gt = 0
        
        for img_id in gt_by_image.keys():
            gt_boxes = gt_by_image[img_id]
            pred_boxes = pred_by_image.get(img_id, [])
            
            total_gt += len(gt_boxes)
            
            if not pred_boxes:
                continue
            
            # Score ì„ê³„ê°’ (0.5) ì ìš©
            pred_boxes = [p for p in pred_boxes if p['score'] >= 0.5]
            
            gt_matched = [False] * len(gt_boxes)
            
            for pred in pred_boxes:
                best_iou = 0
                best_gt_idx = -1
                
                for gt_idx, gt in enumerate(gt_boxes):
                    if gt_matched[gt_idx]:
                        continue
                    
                    iou = self.calculate_bbox_iou(pred['bbox'], gt['bbox'])
                    if iou > best_iou:
                        best_iou = iou
                        best_gt_idx = gt_idx
                
                if best_iou >= iou_thresh and best_gt_idx >= 0:
                    gt_matched[best_gt_idx] = True
                    total_tp += 1
                else:
                    total_fp += 1
        
        precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0
        recall = total_tp / total_gt if total_gt > 0 else 0.0
        
        return precision, recall
    
    def calculate_bbox_iou(self, box1: List[float], box2: List[float]) -> float:
        """ë‘ ë°”ìš´ë”© ë°•ìŠ¤ì˜ IoU ê³„ì‚°"""
        x1 = max(box1[0], box2[0])
        y1 = max(box1[1], box2[1])
        x2 = min(box1[2], box2[2])
        y2 = min(box1[3], box2[3])
        
        if x2 <= x1 or y2 <= y1:
            return 0.0
        
        intersection = (x2 - x1) * (y2 - y1)
        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
        union = area1 + area2 - intersection
        
        return intersection / union if union > 0 else 0.0
    
    def log_inference_images(self, val_loader: DataLoader, epoch: int, num_images: int = 5):
        """ì¶”ë¡  ê²°ê³¼ ì´ë¯¸ì§€ë¥¼ WandBì— ë¡œê¹…"""
        if not self.use_wandb:
            return
        
        self.student.model.eval()
        logged_images = 0
        
        print(f"\nğŸ–¼ï¸ ì¶”ë¡  ê²°ê³¼ ì´ë¯¸ì§€ ìƒì„± ì¤‘...")
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(val_loader):
                if logged_images >= num_images:
                    break
                
                # ë°°ì¹˜ íŒŒì‹±
                if isinstance(batch, dict):
                    images = batch.get('img', batch.get('image'))
                    targets = batch
                elif isinstance(batch, (tuple, list)) and len(batch) >= 2:
                    images, targets = batch[0], batch[1]
                else:
                    continue
                
                if images is None:
                    continue
                
                images = images.float() / 255.0 if images.dtype == torch.uint8 else images
                images = images.to(self.device)
                
                try:
                    # Student ì¶”ë¡ 
                    results = self.student.model(images)
                    
                    # ë°°ì¹˜ì˜ ê° ì´ë¯¸ì§€ ì²˜ë¦¬
                    for i in range(min(images.shape[0], num_images - logged_images)):
                        img_tensor = images[i]
                        result = results[i] if i < len(results) else None
                        
                        # ì´ë¯¸ì§€ë¥¼ numpyë¡œ ë³€í™˜
                        img_np = img_tensor.cpu().permute(1, 2, 0).numpy()
                        img_np = (img_np * 255).astype(np.uint8)
                        
                        # matplotlib figure ìƒì„±
                        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))
                        
                        # ì›ë³¸ ì´ë¯¸ì§€ (GT í¬í•¨)
                        ax1.imshow(img_np)
                        ax1.set_title(f'Ground Truth (Epoch {epoch})', fontsize=14)
                        ax1.axis('off')
                        
                        # GT ë°•ìŠ¤ ê·¸ë¦¬ê¸°
                        if isinstance(targets, dict) and 'bboxes' in targets:
                            batch_idx_tensor = targets.get('batch_idx', torch.arange(images.shape[0]))
                            bboxes = targets['bboxes']
                            
                            img_mask = batch_idx_tensor == (batch_idx * images.shape[0] + i)
                            if img_mask.any():
                                img_bboxes = bboxes[img_mask].cpu().numpy()
                                h, w = img_np.shape[:2]
                                
                                for bbox in img_bboxes:
                                    # YOLO format to pixel coordinates
                                    x1 = (bbox[0] - bbox[2]/2) * w
                                    y1 = (bbox[1] - bbox[3]/2) * h
                                    box_w = bbox[2] * w
                                    box_h = bbox[3] * h
                                    
                                    rect = patches.Rectangle(
                                        (x1, y1), box_w, box_h,
                                        linewidth=2, edgecolor='green', 
                                        facecolor='none', label='GT'
                                    )
                                    ax1.add_patch(rect)
                        
                        # ì˜ˆì¸¡ ê²°ê³¼ ì´ë¯¸ì§€
                        ax2.imshow(img_np)
                        ax2.set_title(f'Student Predictions (Epoch {epoch})', fontsize=14)
                        ax2.axis('off')
                        
                        # ì˜ˆì¸¡ ë°•ìŠ¤ ê·¸ë¦¬ê¸°
                        if result is not None and hasattr(result, 'boxes') and result.boxes is not None:
                            boxes = result.boxes
                            if len(boxes) > 0:
                                pred_boxes = boxes.xyxy.cpu().numpy()
                                pred_scores = boxes.conf.cpu().numpy()
                                
                                for j, (box, score) in enumerate(zip(pred_boxes, pred_scores)):
                                    if score > 0.3:  # ì‹ ë¢°ë„ ì„ê³„ê°’
                                        x1, y1, x2, y2 = box
                                        box_w = x2 - x1
                                        box_h = y2 - y1
                                        
                                        color = 'red' if score > 0.7 else 'orange' if score > 0.5 else 'yellow'
                                        rect = patches.Rectangle(
                                            (x1, y1), box_w, box_h,
                                            linewidth=2, edgecolor=color,
                                            facecolor='none'
                                        )
                                        ax2.add_patch(rect)
                                        
                                        # ì ìˆ˜ í‘œì‹œ
                                        ax2.text(x1, y1-5, f'{score:.2f}', 
                                                color=color, fontsize=10, fontweight='bold')
                        
                        plt.tight_layout()
                        
                        # WandBì— ë¡œê¹…
                        wandb.log({
                            f"inference_images/epoch_{epoch}_img_{logged_images}": wandb.Image(fig),
                            "epoch": epoch
                        })
                        
                        plt.close(fig)
                        logged_images += 1
                        
                        if logged_images >= num_images:
                            break
                
                except Exception as e:
                    print(f"âš ï¸ ì´ë¯¸ì§€ ë¡œê¹… ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
        
        print(f"âœ… {logged_images}ê°œ ì¶”ë¡  ì´ë¯¸ì§€ WandB ë¡œê¹… ì™„ë£Œ")
