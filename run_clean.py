"""
ê°„ê²°í•œ ì¶œë ¥ìœ¼ë¡œ Knowledge Distillation ì‹¤í–‰
"""

import torch
import time
import sys
import os
from yolo_distillation.models.figma_ui_distillation import FigmaUIDistillation


def run_distillation_clean():
    """ë””ë²„ê¹… ì¶œë ¥ì„ ìµœì†Œí™”í•œ KD ì‹¤í–‰"""
    
    # ì„¤ì •
    config = {
        'teacher_model': './weight/best_forest.pt',
        'student_model': 'yolo11s.yaml',
        'data_yaml': './data/yolo_dataset_webforest 3/data.yaml',
        'epochs': 10,
        'batch_size': 16,
        'learning_rate': 0.001,
        'num_workers': 2
    }
    
    print("ğŸš€ Figma UI Knowledge Distillation ì‹œì‘")
    print("=" * 50)
    
    distiller = None
    try:
        # ëª¨ë¸ ì´ˆê¸°í™” (ë””ë²„ê¹… ë¹„í™œì„±í™”)
        print("ğŸ“¦ ëª¨ë¸ ë¡œë”© ì¤‘...")
        distiller = FigmaUIDistillation(
            teacher_model=config['teacher_model'],
            student_model=config['student_model'], 
            data_yaml=config['data_yaml'],
            use_wandb=True
        )
        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        print(f"   Teacher: {distiller.teacher.model.model[-1].nc if hasattr(distiller.teacher.model.model[-1], 'nc') else 'N/A'} classes")
        print(f"   Student: {distiller.student.model.model[-1].nc if hasattr(distiller.student.model.model[-1], 'nc') else 'N/A'} classes")
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        print("ğŸ”„ ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì¬ì‹œë„...")
        try:
            distiller = FigmaUIDistillation(
                teacher_model='yolov11l.pt',
                student_model='yolov11s.yaml',
                data_yaml=config['data_yaml'],
                use_wandb=False
            )
            print("âœ… ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        except Exception as e2:
            print(f"âŒ ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œë„ ì‹¤íŒ¨: {e2}")
            return
    
    if distiller is None:
        print("âŒ ëª¨ë¸ì„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    try:
        # í•™ìŠµ ì‹œì‘
        print(f"\nğŸ¯ í•™ìŠµ ì‹œì‘ ({config['epochs']} epochs)")
        print("-" * 30)
        
        best_map = distiller.train(
            epochs=config['epochs'],
            batch_size=config['batch_size'],
            learning_rate=config['learning_rate'],
            num_workers=config['num_workers']
        )
        
        print("\nğŸ‰ í•™ìŠµ ì™„ë£Œ!")
        print(f"ğŸ“Š Best mAP: {best_map:.4f}")
        
    except Exception as e:
        print(f"âŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜: {e}")
        best_map = 0.0
    
    # ì„±ëŠ¥ ìš”ì•½
    print("\n" + "=" * 50)
    print("ğŸ“ˆ Knowledge Distillation ê²°ê³¼ ìš”ì•½")
    print("-" * 30)
    print(f"Teacher: YOLOv11-l (25M params)")
    print(f"Student: YOLOv11-s (9M params) - mAP: {best_map:.4f}")
    print(f"ì••ì¶•ë¥ : ~62% íŒŒë¼ë¯¸í„° ê°ì†Œ")
    print(f"WandB: https://wandb.ai/ ì—ì„œ ìƒì„¸ ê²°ê³¼ í™•ì¸")


if __name__ == "__main__":
    run_distillation_clean()
